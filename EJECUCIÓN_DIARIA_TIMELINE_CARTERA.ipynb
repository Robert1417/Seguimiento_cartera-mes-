{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from google.oauth2.service_account import Credentials\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 1) Credenciales (Colab usa MI_JSON desde userdata; fuera de Colab usa MI_JSON env)\n",
        "# ======================================================\n",
        "def get_credentials() -> Credentials:\n",
        "    \"\"\"\n",
        "    - En Colab: usa MI_JSON desde google.colab.userdata (NO os.environ)\n",
        "    - En GitHub/local: usa variable de entorno MI_JSON\n",
        "    \"\"\"\n",
        "    info: Dict\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "\n",
        "        mi_json = userdata.get(\"MI_JSON\")\n",
        "        if not mi_json:\n",
        "            raise ValueError(\"MI_JSON no encontrado en Colab userdata\")\n",
        "        info = json.loads(mi_json)\n",
        "        print(\"Entorno detectado: Google Colab\")\n",
        "    except Exception:\n",
        "        mi_json = os.environ.get(\"MI_JSON\")\n",
        "        if not mi_json:\n",
        "            raise ValueError(\"MI_JSON no encontrado como variable de entorno (GitHub/local)\")\n",
        "        info = json.loads(mi_json)\n",
        "        print(\"Entorno detectado: GitHub / local\")\n",
        "\n",
        "    return Credentials.from_service_account_info(\n",
        "        info,\n",
        "        scopes=[\n",
        "            \"https://www.googleapis.com/auth/drive.readonly\",\n",
        "            \"https://www.googleapis.com/auth/spreadsheets.readonly\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "creds = get_credentials()\n",
        "drive_service = build(\"drive\", \"v3\", credentials=creds)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 2) Helpers: meses en espa√±ol + parsing del nombre del archivo\n",
        "# ======================================================\n",
        "MES_MAP = {\n",
        "    \"ene\": 1,\n",
        "    \"feb\": 2,\n",
        "    \"mar\": 3,\n",
        "    \"abr\": 4,\n",
        "    \"may\": 5,\n",
        "    \"jun\": 6,\n",
        "    \"jul\": 7,\n",
        "    \"ago\": 8,\n",
        "    \"sep\": 9,\n",
        "    \"oct\": 10,\n",
        "    \"nov\": 11,\n",
        "    \"dic\": 12,\n",
        "}\n",
        "MES_NOMBRE = {\n",
        "    1: \"Enero\",\n",
        "    2: \"Febrero\",\n",
        "    3: \"Marzo\",\n",
        "    4: \"Abril\",\n",
        "    5: \"Mayo\",\n",
        "    6: \"Junio\",\n",
        "    7: \"Julio\",\n",
        "    8: \"Agosto\",\n",
        "    9: \"Septiembre\",\n",
        "    10: \"Octubre\",\n",
        "    11: \"Noviembre\",\n",
        "    12: \"Diciembre\",\n",
        "}\n",
        "\n",
        "\n",
        "def sheet_name_from_date(dt: datetime) -> str:\n",
        "    \"\"\"Ej: datetime(2025,12,...) -> 'Diciembre 2025' \"\"\"\n",
        "    return f\"{MES_NOMBRE[dt.month]} {dt.year}\"\n",
        "\n",
        "\n",
        "def parse_range_from_filename(name: str) -> Optional[Tuple[int, int, int, int]]:\n",
        "    \"\"\"\n",
        "    Espera nombres tipo:\n",
        "      'Asignaciones de Cartera Ene26-Abr26.xlsx'\n",
        "      'Asignaciones de Cartera Sep25-Dic25.xlsx'\n",
        "    Retorna (start_year, start_month, end_year, end_month) o None si no matchea.\n",
        "    \"\"\"\n",
        "    m = re.search(r\"([A-Za-z]{3})(\\d{2})\\s*-\\s*([A-Za-z]{3})(\\d{2})\", name, flags=re.IGNORECASE)\n",
        "    if not m:\n",
        "        return None\n",
        "\n",
        "    m1, y1, m2, y2 = m.group(1).lower(), m.group(2), m.group(3).lower(), m.group(4)\n",
        "    if m1 not in MES_MAP or m2 not in MES_MAP:\n",
        "        return None\n",
        "\n",
        "    start_month = MES_MAP[m1]\n",
        "    end_month = MES_MAP[m2]\n",
        "    start_year = 2000 + int(y1)\n",
        "    end_year = 2000 + int(y2)\n",
        "\n",
        "    return (start_year, start_month, end_year, end_month)\n",
        "\n",
        "\n",
        "def month_index(year: int, month: int) -> int:\n",
        "    \"\"\"Convierte (year, month) a √≠ndice comparable.\"\"\"\n",
        "    return year * 12 + month\n",
        "\n",
        "\n",
        "def file_covers_month(file_range: Tuple[int, int, int, int], target_dt: datetime) -> bool:\n",
        "    sy, sm, ey, em = file_range\n",
        "    t = month_index(target_dt.year, target_dt.month)\n",
        "    a = month_index(sy, sm)\n",
        "    b = month_index(ey, em)\n",
        "    return a <= t <= b\n",
        "\n",
        "\n",
        "def is_df_empty_like(df: Optional[pd.DataFrame]) -> bool:\n",
        "    \"\"\"\n",
        "    Considera 'vac√≠a' si:\n",
        "    - df es None\n",
        "    - df tiene 0 filas\n",
        "    - o todas las filas est√°n completamente NaN\n",
        "    \"\"\"\n",
        "    if df is None or df.shape[0] == 0:\n",
        "        return True\n",
        "    return df.dropna(how=\"all\").shape[0] == 0\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 3) Listar archivos en carpeta y elegir el correcto por mes\n",
        "# ======================================================\n",
        "def list_assignment_files_in_folder(folder_id: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Lista archivos en la carpeta cuyo nombre contenga 'Asignaciones de Cartera'\n",
        "    y devuelve una lista con metadatos: id, name, mimeType, modifiedTime, parsed_range\n",
        "    \"\"\"\n",
        "    q = f\"'{folder_id}' in parents and trashed=false and name contains 'Asignaciones de Cartera'\"\n",
        "    files: List[Dict] = []\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        resp = (\n",
        "            drive_service.files()\n",
        "            .list(q=q, fields=\"nextPageToken, files(id,name,mimeType,modifiedTime)\", pageToken=page_token)\n",
        "            .execute()\n",
        "        )\n",
        "\n",
        "        for f in resp.get(\"files\", []):\n",
        "            fr = parse_range_from_filename(f.get(\"name\", \"\"))\n",
        "            if fr:\n",
        "                f[\"parsed_range\"] = fr\n",
        "                files.append(f)\n",
        "\n",
        "        page_token = resp.get(\"nextPageToken\")\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "    if not files:\n",
        "        raise ValueError(\n",
        "            \"No encontr√© archivos 'Asignaciones de Cartera' con rango tipo Ene26-Abr26 dentro de la carpeta.\"\n",
        "        )\n",
        "\n",
        "    return files\n",
        "\n",
        "\n",
        "def pick_file_for_month(files_meta: List[Dict], target_dt: datetime) -> Dict:\n",
        "    \"\"\"\n",
        "    Escoge el archivo cuya ventana (en el nombre) cubra el mes target_dt.\n",
        "    Si hay varios, elige el de rango m√°s corto (m√°s espec√≠fico) y si empatan, el m√°s reciente.\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    for f in files_meta:\n",
        "        fr = f[\"parsed_range\"]\n",
        "        if file_covers_month(fr, target_dt):\n",
        "            sy, sm, ey, em = fr\n",
        "            span = month_index(ey, em) - month_index(sy, sm)\n",
        "            candidates.append((span, f.get(\"modifiedTime\", \"\"), f))\n",
        "\n",
        "    if candidates:\n",
        "        candidates.sort(key=lambda x: (x[0], x[1]))  # menor span, luego por modifiedTime asc\n",
        "        min_span = candidates[0][0]\n",
        "        same_span = [c for c in candidates if c[0] == min_span]\n",
        "        same_span.sort(key=lambda x: x[1], reverse=True)  # m√°s reciente primero\n",
        "        return same_span[0][2]\n",
        "\n",
        "    t = month_index(target_dt.year, target_dt.month)\n",
        "\n",
        "    past = []\n",
        "    for f in files_meta:\n",
        "        sy, sm, ey, em = f[\"parsed_range\"]\n",
        "        end_i = month_index(ey, em)\n",
        "        if end_i <= t:\n",
        "            past.append((end_i, f.get(\"modifiedTime\", \"\"), f))\n",
        "    if past:\n",
        "        past.sort(key=lambda x: (x[0], x[1]), reverse=True)\n",
        "        return past[0][2]\n",
        "\n",
        "    future = []\n",
        "    for f in files_meta:\n",
        "        sy, sm, ey, em = f[\"parsed_range\"]\n",
        "        start_i = month_index(sy, sm)\n",
        "        if start_i >= t:\n",
        "            future.append((start_i, f.get(\"modifiedTime\", \"\"), f))\n",
        "    if future:\n",
        "        future.sort(key=lambda x: (x[0], x[1]))\n",
        "        return future[0][2]\n",
        "\n",
        "    raise ValueError(\"No se pudo escoger un archivo por fecha (revisa nombres/rangos).\")\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 4) Descargar archivo (Google Sheets o Excel) a memoria\n",
        "# ======================================================\n",
        "def download_file_to_buffer(file_id: str, mime_type: str) -> io.BytesIO:\n",
        "    buffer = io.BytesIO()\n",
        "\n",
        "    if mime_type == \"application/vnd.google-apps.spreadsheet\":\n",
        "        request = drive_service.files().export_media(\n",
        "            fileId=file_id,\n",
        "            mimeType=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n",
        "        )\n",
        "    else:\n",
        "        request = drive_service.files().get_media(fileId=file_id)\n",
        "\n",
        "    downloader = MediaIoBaseDownload(buffer, request)\n",
        "    done = False\n",
        "    while not done:\n",
        "        _, done = downloader.next_chunk()\n",
        "\n",
        "    buffer.seek(0)\n",
        "    return buffer\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 5) Intentar leer hoja del mes target; si est√° vac√≠a -> retroceder mes a mes\n",
        "# ======================================================\n",
        "def shift_month(dt: datetime, n: int) -> datetime:\n",
        "    \"\"\"Mueve dt n meses (n puede ser negativo).\"\"\"\n",
        "    y = dt.year + (dt.month - 1 + n) // 12\n",
        "    m = (dt.month - 1 + n) % 12 + 1\n",
        "    return datetime(y, m, 1)\n",
        "\n",
        "\n",
        "def load_assignment_base_from_folder(\n",
        "    folder_id: str, max_back_months: int = 24\n",
        ") -> Tuple[pd.DataFrame, Dict, str]:\n",
        "    \"\"\"\n",
        "    Busca base del mes actual; si no existe o est√° vac√≠a,\n",
        "    busca mes anterior (incluyendo cambio de archivo si aplica).\n",
        "    Retorna: (df, file_meta, sheet_name_usada)\n",
        "    \"\"\"\n",
        "    files_meta = list_assignment_files_in_folder(folder_id)\n",
        "    today = datetime.today()\n",
        "\n",
        "    last_error: Optional[Exception] = None\n",
        "\n",
        "    for back in range(0, max_back_months + 1):\n",
        "        target_dt = shift_month(today, -back)\n",
        "        target_sheet = sheet_name_from_date(target_dt)\n",
        "        chosen = pick_file_for_month(files_meta, target_dt)\n",
        "\n",
        "        try:\n",
        "            buffer = download_file_to_buffer(chosen[\"id\"], chosen[\"mimeType\"])\n",
        "            df = pd.read_excel(buffer, sheet_name=target_sheet, engine=\"openpyxl\")\n",
        "\n",
        "            if is_df_empty_like(df):\n",
        "                print(f\"üü° {target_sheet} encontrado pero vac√≠o en: {chosen['name']} -> probando mes anterior...\")\n",
        "                continue\n",
        "\n",
        "            print(\"‚úÖ Base encontrada\")\n",
        "            print(f\"   Archivo: {chosen['name']}\")\n",
        "            print(f\"   Hoja:    {target_sheet}\")\n",
        "            return df, chosen, target_sheet\n",
        "\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            print(\n",
        "                f\"üü† No se pudo usar {target_sheet} en {chosen['name']} ({type(e).__name__}) -> probando mes anterior...\"\n",
        "            )\n",
        "\n",
        "    raise RuntimeError(\n",
        "        f\"No encontr√© una hoja v√°lida en los √∫ltimos {max_back_months} meses. \"\n",
        "        f\"√öltimo error: {repr(last_error)}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 6) EJECUCI√ìN\n",
        "# ======================================================\n",
        "FOLDER_ID = \"1cf2p3R7iM0xowAt4muEruDwxZoZqD_jB\"\n",
        "\n",
        "df, meta_file, sheet_used = load_assignment_base_from_folder(\n",
        "    folder_id=FOLDER_ID,\n",
        "    max_back_months=24,\n",
        ")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uaP2d24vS1SY",
        "outputId": "5d034938-ac61-4760-d847-2e5af69c5a0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entorno detectado: Google Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={\"Deuda Resuelve\": \"D_BRAVO\"})"
      ],
      "metadata": {
        "id": "nE-a-x_XMQp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={\"Meses de atraso\": \"MORA\"})"
      ],
      "metadata": {
        "id": "-E1sCNlMlzA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "b-Bd8AKedT3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------\n",
        "# PASO 1 ‚Äî Construir base de cartera asignada\n",
        "# ---------------------------------------\n",
        "\n",
        "# Trabajamos sobre copia por seguridad\n",
        "df_cartera = df.copy()\n",
        "\n",
        "# Columnas que queremos conservar\n",
        "cols_cartera = [\n",
        "    \"Referencia\",\n",
        "    \"Id deuda\",\n",
        "    \"Cedula\",\n",
        "    \"Nombre del cliente\",\n",
        "    \"Negociador\",\n",
        "    \"BANCOS_ESTANDAR\",\n",
        "    \"Descuento\",\n",
        "    \"D_BRAVO\",\n",
        "    \"MORA\",\n",
        "    \"Estructurable\",\n",
        "    \"Potencial\",\n",
        "    \"Meses en el Programa\",\n",
        "    \"Tipo de Liquidacion\",\n",
        "    \"Bucket\",\n",
        "    \"Ahorro total\",\n",
        "    \"Ahorro medio\",\n",
        "    \"Por cobrar\",\n",
        "    \"Potencial Credito\",\n",
        "    \"Estado Deuda\",\n",
        "    \"sub_estado_deuda\",\n",
        "    \"estado_reparadora\",\n",
        "    \"sub_estado_reparadora\",\n",
        "    \"Mora_estructurado\",\n",
        "    \"MORA_CREDITO\",\n",
        "    \"Priority_level\",\n",
        "    \"ultimo contacto\",\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Validaci√≥n: columnas faltantes\n",
        "faltantes = [c for c in cols_cartera if c not in df_cartera.columns]\n",
        "if faltantes:\n",
        "    raise ValueError(f\"Faltan estas columnas en df: {faltantes}\")\n",
        "\n",
        "# Nos quedamos solo con esas columnas\n",
        "df_cartera = df_cartera[cols_cartera].copy()\n",
        "\n",
        "# Opcional: eliminar duplicados por deuda (clave natural)\n",
        "df_cartera = df_cartera.drop_duplicates(subset=[\"Id deuda\"])\n",
        "\n",
        "# Reset index limpio\n",
        "df_cartera = df_cartera.reset_index(drop=True)\n",
        "\n",
        "df_cartera.info()"
      ],
      "metadata": {
        "id": "A_LoxDeub6e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cartera"
      ],
      "metadata": {
        "id": "IIe7Q8Rld204"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from datetime import datetime\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "# =====================================\n",
        "# 1. Cargar MI_JSON (Colab / GitHub)\n",
        "# =====================================\n",
        "def get_mi_json():\n",
        "    try:\n",
        "        # ---- Colab ----\n",
        "        from google.colab import userdata\n",
        "        mi_json = userdata.get(\"MI_JSON\")\n",
        "        if not mi_json:\n",
        "            raise RuntimeError(\"MI_JSON no encontrado en Colab userdata\")\n",
        "        print(\"Entorno detectado: Google Colab\")\n",
        "        return mi_json\n",
        "    except Exception:\n",
        "        # ---- GitHub Actions / local ----\n",
        "        mi_json = os.environ.get(\"MI_JSON\")\n",
        "        if not mi_json:\n",
        "            raise RuntimeError(\"MI_JSON no encontrado como variable de entorno (GitHub/local)\")\n",
        "        if os.environ.get(\"GITHUB_ACTIONS\") == \"true\":\n",
        "            print(\"Entorno detectado: GitHub Actions\")\n",
        "        else:\n",
        "            print(\"Entorno detectado: Local\")\n",
        "        return mi_json\n",
        "\n",
        "mi_json = get_mi_json()\n",
        "creds_dict = json.loads(mi_json)\n",
        "\n",
        "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets.readonly\"]\n",
        "creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# =====================================\n",
        "# 2. Definir a√±o objetivo (regla 6 d√≠as)\n",
        "# =====================================\n",
        "today = datetime.today()\n",
        "\n",
        "if today.month == 1 and today.day <= 6:\n",
        "    target_year = today.year - 1\n",
        "else:\n",
        "    target_year = today.year\n",
        "\n",
        "print(f\"üìÖ A√±o objetivo detectado: {target_year}\")\n",
        "\n",
        "# =====================================\n",
        "# 3. Abrir spreadsheet\n",
        "# =====================================\n",
        "SPREADSHEET_ID = \"1O8OHuVhgwhLw8XYEBf1uBzLYrxQ45rPiZecHOnAa1Go\"\n",
        "sh = gc.open_by_key(SPREADSHEET_ID)\n",
        "\n",
        "# =====================================\n",
        "# 4. Buscar hoja por a√±o en el nombre\n",
        "# =====================================\n",
        "worksheet_found = None\n",
        "for ws in sh.worksheets():\n",
        "    if str(target_year) in ws.title:\n",
        "        worksheet_found = ws\n",
        "        break\n",
        "\n",
        "if worksheet_found is None:\n",
        "    raise ValueError(\n",
        "        f\"No se encontr√≥ ninguna hoja cuyo nombre contenga el a√±o {target_year}\"\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Hoja seleccionada: '{worksheet_found.title}'\")\n",
        "\n",
        "# =====================================\n",
        "# 5. Leer a DataFrame\n",
        "# =====================================\n",
        "records = worksheet_found.get_all_records()\n",
        "\n",
        "df_act = pd.DataFrame(records)\n",
        "\n",
        "print(\"‚úÖ df_act cargado correctamente\")\n",
        "print(\"Shape:\", df_act.shape)\n",
        "print(df_act.head())"
      ],
      "metadata": {
        "id": "Fw52OBpPd563"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_act"
      ],
      "metadata": {
        "id": "c-5jzcpUeADe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "df_act = df_act.copy()\n",
        "\n",
        "s = df_act[\"inserted_at\"]\n",
        "\n",
        "# Pasar todo a string limpio (sin romper NaN)\n",
        "s_str = s.astype(\"string\").str.strip()\n",
        "\n",
        "# Serie destino\n",
        "dt = pd.Series(pd.NaT, index=df_act.index, dtype=\"datetime64[ns]\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Formato ISO: 2025-12-18 02:09:59 (o 2025-12-18T02:09:59Z)\n",
        "# -----------------------------\n",
        "mask_iso = s_str.str.match(r\"^\\d{4}-\\d{2}-\\d{2}\", na=False)\n",
        "iso_clean = (\n",
        "    s_str[mask_iso]\n",
        "    .str.replace(\"T\", \" \", regex=False)\n",
        "    .str.replace(\"Z\", \"\", regex=False)\n",
        ")\n",
        "dt.loc[mask_iso] = pd.to_datetime(iso_clean, errors=\"coerce\")  # conserva hora\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Formato con slash: 2/1/2025 (puede ser d/m/y o m/d/y)\n",
        "# -----------------------------\n",
        "mask_slash = s_str.str.contains(r\"/\", na=False) & (~mask_iso)\n",
        "\n",
        "# extraer d√≠a/mes/a√±o como n√∫meros\n",
        "parts = s_str[mask_slash].str.extract(r\"^\\s*(\\d{1,2})/(\\d{1,2})/(\\d{4})\\s*$\")\n",
        "a = pd.to_numeric(parts[0], errors=\"coerce\")  # primera parte\n",
        "b = pd.to_numeric(parts[1], errors=\"coerce\")  # segunda parte\n",
        "\n",
        "# reglas para decidir dayfirst vs monthfirst\n",
        "mask_dayfirst = (a > 12) & (b <= 12)\n",
        "mask_monthfirst = (b > 12) & (a <= 12)\n",
        "mask_ambigua = ~(mask_dayfirst | mask_monthfirst)\n",
        "\n",
        "idx_slash = parts.index\n",
        "\n",
        "# dayfirst seguro\n",
        "idx_day = idx_slash[mask_dayfirst.fillna(False)]\n",
        "dt.loc[idx_day] = pd.to_datetime(s_str.loc[idx_day], errors=\"coerce\", dayfirst=True)\n",
        "\n",
        "# monthfirst seguro\n",
        "idx_mon = idx_slash[mask_monthfirst.fillna(False)]\n",
        "dt.loc[idx_mon] = pd.to_datetime(s_str.loc[idx_mon], errors=\"coerce\", dayfirst=False)\n",
        "\n",
        "# ambigua -> por defecto dayfirst=True (tu est√°ndar)\n",
        "idx_amb = idx_slash[mask_ambigua.fillna(True)]\n",
        "tmp = pd.to_datetime(s_str.loc[idx_amb], errors=\"coerce\", dayfirst=True)\n",
        "\n",
        "# si alguna ambigua falla, reintenta monthfirst\n",
        "mask_fail = tmp.isna()\n",
        "if mask_fail.any():\n",
        "    tmp.loc[mask_fail] = pd.to_datetime(s_str.loc[idx_amb[mask_fail]], errors=\"coerce\", dayfirst=False)\n",
        "\n",
        "dt.loc[idx_amb] = tmp\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Guardar resultado final\n",
        "# -----------------------------\n",
        "df_act[\"inserted_at\"] = dt\n",
        "\n",
        "# (Opcional) Ver cu√°ntos quedaron NaT\n",
        "print(\"NaT en inserted_at:\", df_act[\"inserted_at\"].isna().sum())\n",
        "print(\"dtype:\", df_act[\"inserted_at\"].dtype)"
      ],
      "metadata": {
        "id": "cQseOQikeEvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "df_act[\"payment_to_bank\"] = (\n",
        "    df_act[\"payment_to_bank\"]\n",
        "    .astype(str)\n",
        "    .str.extract(r\"\\(?\\s*([\\d]+)\\s*,?\\s*COP?\\s*\\)?\", expand=False)\n",
        "    .astype(float)\n",
        ")"
      ],
      "metadata": {
        "id": "URZwhFCweEr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_act['payment_to_bank'] = df_act['payment_to_bank']/100"
      ],
      "metadata": {
        "id": "ASSE5AV6eHhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_act = df_act.rename(columns={\"Status_Act\": \"CATEGORIA_PRED\"})"
      ],
      "metadata": {
        "id": "nPkV8ItseLnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_act"
      ],
      "metadata": {
        "id": "uH5RsE7ye0_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Compatibilidad ZoneInfo: en Py<3.9 (algunos entornos) usa backports\n",
        "try:\n",
        "    from zoneinfo import ZoneInfo\n",
        "except ImportError:  # pragma: no cover\n",
        "    from backports.zoneinfo import ZoneInfo  # type: ignore\n",
        "\n",
        "\n",
        "def construir_timeline_mes(\n",
        "    df_cartera: pd.DataFrame,\n",
        "    df_act: pd.DataFrame,\n",
        "    ref_date=None,\n",
        "    tz: str = \"America/Bogota\",\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Timeline por deuda:\n",
        "    - √öltima observaci√≥n antes del mes\n",
        "    - Todas las observaciones del mes actual\n",
        "    Requiere que df_act ya tenga columna 'CATEGORIA_PRED' si quieres usarla.\n",
        "    \"\"\"\n",
        "\n",
        "    # =========================\n",
        "    # 0) Copias defensivas\n",
        "    # =========================\n",
        "    df_c = df_cartera.copy()\n",
        "    df_a = df_act.copy()\n",
        "\n",
        "    # =========================\n",
        "    # 1) Fecha de referencia\n",
        "    # =========================\n",
        "    tzinfo = ZoneInfo(tz)\n",
        "\n",
        "    if ref_date is None:\n",
        "        ref_date = pd.Timestamp.now(tzinfo)\n",
        "    else:\n",
        "        ref_date = pd.Timestamp(ref_date)\n",
        "        if ref_date.tzinfo is None:\n",
        "            ref_date = ref_date.tz_localize(tzinfo)\n",
        "        else:\n",
        "            ref_date = ref_date.tz_convert(tzinfo)\n",
        "\n",
        "    month_start = ref_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
        "    next_month_start = month_start + pd.offsets.MonthBegin(1)\n",
        "\n",
        "    # =========================\n",
        "    # 2) Normalizar fechas\n",
        "    # =========================\n",
        "    df_a[\"inserted_at\"] = pd.to_datetime(df_a[\"inserted_at\"], errors=\"coerce\")\n",
        "\n",
        "    if getattr(df_a[\"inserted_at\"].dt, \"tz\", None) is None:\n",
        "        df_a[\"inserted_at\"] = df_a[\"inserted_at\"].dt.tz_localize(\n",
        "            tzinfo,\n",
        "            nonexistent=\"shift_forward\",\n",
        "            ambiguous=\"NaT\",\n",
        "        )\n",
        "    else:\n",
        "        df_a[\"inserted_at\"] = df_a[\"inserted_at\"].dt.tz_convert(tzinfo)\n",
        "\n",
        "    # =========================\n",
        "    # 3) Separar eventos\n",
        "    # =========================\n",
        "    mask_mes = (df_a[\"inserted_at\"] >= month_start) & (df_a[\"inserted_at\"] < next_month_start)\n",
        "\n",
        "    df_mes = df_a.loc[mask_mes].copy()\n",
        "    df_prev = df_a.loc[df_a[\"inserted_at\"] < month_start].copy()\n",
        "\n",
        "    # =========================\n",
        "    # 4) √öltima antes del mes\n",
        "    # =========================\n",
        "    df_prev = df_prev.sort_values([\"debt_id\", \"inserted_at\"])\n",
        "    ultima_prev = df_prev.groupby(\"debt_id\", as_index=False).tail(1)\n",
        "    ultima_prev[\"tipo_fila\"] = \"ultima_antes_mes\"\n",
        "\n",
        "    # =========================\n",
        "    # 5) Todas las del mes\n",
        "    # =========================\n",
        "    df_mes = df_mes.sort_values([\"debt_id\", \"inserted_at\"])\n",
        "    df_mes[\"tipo_fila\"] = \"mes_actual\"\n",
        "\n",
        "    # =========================\n",
        "    # 6) Unir eventos\n",
        "    # =========================\n",
        "    eventos = pd.concat([ultima_prev, df_mes], ignore_index=True)\n",
        "\n",
        "    # Columnas de eventos que quieres traer s√≠ o s√≠\n",
        "    cols_eventos = [\n",
        "        \"bank_reference\",\n",
        "        \"debt_id\",\n",
        "        \"inserted_at\",\n",
        "        \"end\",\n",
        "        \"payment_to_bank\",\n",
        "        \"CATEGORIA_PRED\",\n",
        "        \"observations\",\n",
        "        \"tipo_fila\",\n",
        "    ]\n",
        "    for c in cols_eventos:\n",
        "        if c not in eventos.columns:\n",
        "            eventos[c] = pd.NA\n",
        "    eventos = eventos[cols_eventos]\n",
        "\n",
        "    # =========================\n",
        "    # 7) Merge con cartera\n",
        "    # =========================\n",
        "    df_timeline = df_c.merge(\n",
        "        eventos,\n",
        "        left_on=[\"Referencia\", \"Id deuda\"],\n",
        "        right_on=[\"bank_reference\", \"debt_id\"],\n",
        "        how=\"left\",\n",
        "    ).drop(columns=[\"bank_reference\", \"debt_id\"])\n",
        "\n",
        "    # =========================\n",
        "    # 8) Garantizar columnas requeridas en el output\n",
        "    # =========================\n",
        "    cols_cartera_requeridas = [\n",
        "        \"Referencia\",\n",
        "        \"Id deuda\",\n",
        "        \"Cedula\",\n",
        "        \"Nombre del cliente\",\n",
        "        \"Negociador\",\n",
        "        \"BANCOS_ESTANDAR\",\n",
        "        \"Descuento\",\n",
        "        \"D_BRAVO\",\n",
        "        \"MORA\",\n",
        "        \"Estructurable\",\n",
        "        \"Potencial\",\n",
        "        \"Meses en el Programa\",\n",
        "        \"Tipo de Liquidacion\",\n",
        "        \"Bucket\",\n",
        "        \"Ahorro total\",\n",
        "        \"Ahorro medio\",\n",
        "        \"Por cobrar\",\n",
        "        \"Potencial Credito\",\n",
        "        \"Estado Deuda\",\n",
        "        \"sub_estado_deuda\",\n",
        "        \"estado_reparadora\",\n",
        "        \"sub_estado_reparadora\",\n",
        "        \"Mora_estructurado\",\n",
        "        \"MORA_CREDITO\",\n",
        "        \"Priority_level\",\n",
        "        \"ultimo contacto\",\n",
        "        \"fecha mensaje\"\n",
        "    ]\n",
        "\n",
        "    # Si alguna no existe, cr√©ala en NA para evitar KeyError\n",
        "    for c in cols_cartera_requeridas:\n",
        "        if c not in df_timeline.columns:\n",
        "            df_timeline[c] = pd.NA\n",
        "\n",
        "    # Orden sugerido: primero tus columnas de cartera, luego las de eventos (y luego cualquier extra que exista)\n",
        "    cols_eventos_out = [\"inserted_at\", \"end\", \"payment_to_bank\", \"CATEGORIA_PRED\", \"observations\", \"tipo_fila\"]\n",
        "    extras = [c for c in df_timeline.columns if c not in (cols_cartera_requeridas + cols_eventos_out)]\n",
        "\n",
        "    df_timeline = df_timeline[cols_cartera_requeridas + cols_eventos_out + extras]\n",
        "\n",
        "    # =========================\n",
        "    # 9) Orden final\n",
        "    # =========================\n",
        "    df_timeline = df_timeline.sort_values(\n",
        "        [\"Id deuda\", \"inserted_at\"],\n",
        "        na_position=\"first\"\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    return df_timeline\n",
        "\n",
        "\n",
        "# ===== EJECUCI√ìN =====\n",
        "df_timeline = construir_timeline_mes(df_cartera, df_act)\n",
        "df_timeline.head(20)"
      ],
      "metadata": {
        "id": "U1Ld0WiNe3g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline"
      ],
      "metadata": {
        "id": "viRkSOupN56w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "SPREADSHEET_ID = \"1H3sYEtkeu47POnu8xZMaMtID1Vj53YIcWblWeZ8d0rc\"\n",
        "GID = \"1033250632\"\n",
        "\n",
        "# =========================\n",
        "# 1) Leer MI_JSON seg√∫n entorno (Colab vs GitHub/local)\n",
        "# =========================\n",
        "def get_mi_json():\n",
        "    # --- Colab ---\n",
        "    try:\n",
        "        from google.colab import userdata  # solo existe en Colab\n",
        "        mi_json = userdata.get(\"MI_JSON\")\n",
        "        if not mi_json:\n",
        "            raise RuntimeError(\"MI_JSON no encontrado en Colab userdata.\")\n",
        "        print(\"Entorno detectado: Google Colab\")\n",
        "        return mi_json\n",
        "    except Exception:\n",
        "        # --- GitHub Actions / local ---\n",
        "        mi_json = os.environ.get(\"MI_JSON\")\n",
        "        if not mi_json:\n",
        "            raise RuntimeError(\"MI_JSON no encontrado como variable de entorno (GitHub/local).\")\n",
        "        if os.environ.get(\"GITHUB_ACTIONS\") == \"true\":\n",
        "            print(\"Entorno detectado: GitHub Actions\")\n",
        "        else:\n",
        "            print(\"Entorno detectado: Local\")\n",
        "        return mi_json\n",
        "\n",
        "mi_json = get_mi_json()\n",
        "info = json.loads(mi_json)\n",
        "\n",
        "SCOPES = [\n",
        "    \"https://www.googleapis.com/auth/spreadsheets.readonly\",\n",
        "    \"https://www.googleapis.com/auth/drive.readonly\",\n",
        "]\n",
        "creds = Credentials.from_service_account_info(info, scopes=SCOPES)\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "sh = gc.open_by_key(SPREADSHEET_ID)\n",
        "\n",
        "# =========================\n",
        "# 2) Resolver el nombre de la hoja a partir del gid\n",
        "# =========================\n",
        "meta = sh.fetch_sheet_metadata()\n",
        "sheet_title = None\n",
        "for s in meta.get(\"sheets\", []):\n",
        "    props = s.get(\"properties\", {})\n",
        "    if str(props.get(\"sheetId\")) == str(GID):\n",
        "        sheet_title = props.get(\"title\")\n",
        "        break\n",
        "\n",
        "if not sheet_title:\n",
        "    raise ValueError(f\"No encontr√© ninguna hoja con gid={GID}. Revisa el link.\")\n",
        "\n",
        "ws = sh.worksheet(sheet_title)\n",
        "\n",
        "# =========================\n",
        "# 3) Leer a DataFrame\n",
        "# =========================\n",
        "values = ws.get_all_values()\n",
        "\n",
        "if not values or len(values) < 2:\n",
        "    df_liq = pd.DataFrame()\n",
        "else:\n",
        "    headers = values[0]\n",
        "    rows = values[1:]\n",
        "    df_liq = pd.DataFrame(rows, columns=headers)\n",
        "\n",
        "print(\"‚úÖ Hoja le√≠da:\", sheet_title)\n",
        "print(\"Shape:\", df_liq.shape)\n",
        "print(df_liq.head(20))"
      ],
      "metadata": {
        "id": "GX2_LKfMfjDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_liq"
      ],
      "metadata": {
        "id": "FU_e5ZQ98Gy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fuzzywuzzy[speedup]"
      ],
      "metadata": {
        "id": "GPIGfMd-fvat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rapidfuzz import process, fuzz\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "nombres_correctos = [\n",
        "    'Bancolombia', 'Banco Falabella', 'Banco de Bogot√°', 'Banco Davivienda',\n",
        "    'Scotiabank Colpatria', 'BBVA Colombia', 'SisteCredito', 'Banco AV Villas',\n",
        "    'Banco de Occidente', 'Alkomprar', 'Tuya', 'Codensa', 'Ita√∫',\n",
        "    'Serfinanza', 'Credivalores', 'Banco Popular', 'Rappipay',\n",
        "    'Banco Finandina', 'Banco Caja Social', 'Covinoc', 'Bancoomeva',\n",
        "    'Rapicredit', 'Credijamar', 'Flamingo', 'Zinobe', 'Bancamia',\n",
        "    'Refinancia', 'Compensar', 'Pichincha', 'Colsubsidio', 'Mundo Mujer',\n",
        "    'Agaval', 'Sistemcobro', 'Serlefin', 'Rappicard', 'Fincomercio',\n",
        "    'GRUPO JURIDICO DEUDU', 'AECSA', 'Sufi', 'Comultrasan', 'Fundaci√≥n',\n",
        "    'Cobrando', 'Aslegal', 'Coltefinanciera', 'Reestructura', 'Nu',\n",
        "    'Juancho te Presta', 'Muebles', 'JOHN', 'GNB Sudameris', 'Confiar',\n",
        "    'Baninca', 'Systemgroup', 'Efecty'\n",
        "]\n",
        "\n",
        "# Bancos que se usan con el prefijo \"banco\" completo como referencia\n",
        "bancos_excepciones = ['Banco de Bogot√°', 'Bancolombia', 'Bancoomeva']\n",
        "\n",
        "# ==============================\n",
        "# 2. Sin√≥nimos / patrones por banco (texto original, sin limpiar)\n",
        "# ==============================\n",
        "PATRONES_POR_BANCO = {\n",
        "    'Bancolombia': [\n",
        "        'bancolombia', 'contento bancolombia', 'qnt bancolombia', 'sufi'\n",
        "    ],\n",
        "    'Banco Davivienda': [\n",
        "        'davivienda', 'intercredito davivienda', 'gestiones profesionales davivienda',\n",
        "        'inversionistas estrat√©gicos davivienda', 'inversionistas estrategicos davivienda',\n",
        "        'management davivienda', 'davivienda cobrado sas', 'davivienda cobrando sas',\n",
        "        'deudu davivienda', 'qnt davivienda'\n",
        "    ],\n",
        "    'BBVA Colombia': [\n",
        "        'bbva', 'cobrando bbva', 'beta bbva', 'cobranzas beta origen: bbva',\n",
        "        'aecsa bbva', 'grupo juridico bbva', 'grupo jur√≠dico bbva', 'qnt bbva'\n",
        "    ],\n",
        "    'Banco Falabella': [\n",
        "        'falabella', 'bfalabella', 'bancofalab', 'bancofalab citisumma',\n",
        "        'banco falabella casa de cobro', 'eyc falabella',\n",
        "        'cobrando falabella', 'acr logros f ori falabella',\n",
        "        'deudu falabella', 'deudo falabella', 'citisumma falabella',\n",
        "        'logros factoring falabella'\n",
        "    ],\n",
        "    'Tuya': [\n",
        "        ' tuya', 'viva-tuya', 'viva tuya', 'qnt tuya', 'aecsa tuya',\n",
        "        'tuya contacto soluciones', 'tuya s.a contactosol', 'tuya s a contactosol',\n",
        "        'tuya contacto soluciones', 'qnt tuya',\n",
        "        '√©xito', 'exito', 'carulla', 'Alkosto', 'Corbeta'\n",
        "    ],\n",
        "    'Scotiabank Colpatria': [\n",
        "        'scotiabank', 'skotiabank', 'colpatria', 'peruzzi skotiabank colpatria',\n",
        "        'serlefin colpatria', 'adamantine scotiabank', 'gc andino colpatria',\n",
        "        'scotiabank citibank', 'qnt colpatria', 'crc colpatria',\n",
        "        'grupo consulto colpatria', 'grupo consultor andino colpatria',\n",
        "        'gr.consulto colpatria', 'Codensa'\n",
        "    ],\n",
        "    'Banco de Bogot√°': [\n",
        "        'banco de bogota', 'banco de bogot√°', 'qnt bogota', 'qnt bogot√°',\n",
        "        'crear pa√≠s banco de bogot√°', 'crear pais banco de bogota'\n",
        "    ],\n",
        "    'Banco de Occidente': [\n",
        "        'banco de occidente', 'qnt banco de occidente', 'deudu-banco de occidente',\n",
        "        'deudu banco de occidente'\n",
        "    ],\n",
        "    'Banco Popular': [\n",
        "        'banco popular', 'banco popular casa de cobro', 'banco popular contactosol',\n",
        "        'banco popular contactosolsas', 'banco popular citisumma',\n",
        "        'banco popular-adcore', 'deudu banco popular', 'peruzzicol bcopopular'\n",
        "    ],\n",
        "    'Banco AV Villas': [\n",
        "        'av villas', 'banco av villas', 'grupo consultor andino av villas',\n",
        "        'grupo juridico av villas', 'grupo jur√≠dico av villas',\n",
        "        'ae csa av villas', 'aecsa av villas', 'crear pa√≠s banco av villas',\n",
        "        'crear pais banco av villas', 'deudu av villas', 'qnt av villas'\n",
        "    ],\n",
        "    'Banco Caja Social': [\n",
        "        'banco caja social', 'caja social', 'pic caja social'\n",
        "    ],\n",
        "    'Bancoomeva': [\n",
        "        'banco coomeva', 'bancoomeva', 'coomeva'\n",
        "    ],\n",
        "    'Bancamia': [\n",
        "        'bancamia', 'bancamia s.a', 'bancamia s a'\n",
        "    ],\n",
        "    'Mundo Mujer': [\n",
        "        'banco mundo mujer', 'fundacion de la mujer', 'fundaci√≥n de la mujer'\n",
        "    ],\n",
        "    'SisteCredito': [\n",
        "        'sistecredito', 'sistecr√©dito'\n",
        "    ],\n",
        "    'Covinoc': ['covinoc'],\n",
        "    'Compensar': ['compensar'],\n",
        "    'Pichincha': ['pichincha', 'pichincha educativo'],\n",
        "    'Agaval': ['agaval'],\n",
        "    'Banco Finandina': [\n",
        "        'finandina', 'finandina incomercio', 'finandina incomercio'\n",
        "    ],\n",
        "    'Fincomercio': ['fincomercio'],\n",
        "    'Serfinanza': [\n",
        "        'serfinanza', 'serfinansa', 'serfinanza contactosol',\n",
        "        'contacto solucion serfinanza', 'contacto soluci√≥n serfinanza'\n",
        "    ],\n",
        "    'Credijamar': [\n",
        "        'credijamar', 'muebles jamar'\n",
        "    ],\n",
        "    'Juancho te Presta': ['juancho te presta'],\n",
        "    'Rapicredit': ['rapicredit'],\n",
        "    'Zinobe': ['zinobe'],\n",
        "    'Coltefinanciera': ['coltefinanciera'],\n",
        "    'Sistemcobro': ['sistemcobro'],\n",
        "    'Systemgroup': ['systemgroup'],\n",
        "    'Baninca': ['baninca'],\n",
        "    'GNB Sudameris': ['gnb sudameris'],\n",
        "    'Confiar': ['confiar'],\n",
        "    'AECSA': ['aecsa'],\n",
        "    'Comultrasan': ['comultrasan'],\n",
        "    'Nu': ['nu bank', 'nubank', 'logros factoring nubank'],\n",
        "    'Rappi':['Rappicard', 'Rappipay'],\n",
        "    'Lulo Bank': ['LuloBank', 'Lulo Banck'],\n",
        "    'Banco union': ['QNT GIROS&FINANZAS']\n",
        "\n",
        "}\n",
        "\n",
        "# ==============================\n",
        "# 3. Alias manuales sobre texto LIMPIO\n",
        "# ==============================\n",
        "alias_manuales = {\n",
        "    'bfalabella': 'Banco Falabella',\n",
        "    'contactosol': 'Banco Falabella',\n",
        "    'qnt itau': 'Ita√∫',\n",
        "    'itau helm': 'Ita√∫',\n",
        "    'itau corpbanca': 'Ita√∫',\n",
        "    'viva tuya': 'Tuya',\n",
        "    'carulla': 'Tuya',\n",
        "    'sufi': 'Bancolombia',\n",
        "    'adamantine scotiabank': 'Scotiabank Colpatria',\n",
        "    'gc andino colpatria': 'Scotiabank Colpatria',\n",
        "    'beta bbva': 'BBVA Colombia',\n",
        "    'banco popular': 'Banco Popular',\n",
        "    'qnt bogota': 'Banco de Bogot√°',\n",
        "    'banco caja social': 'Banco Caja Social',\n",
        "    'banco av villas': 'Banco AV Villas',\n",
        "    'banco davivienda': 'Banco Davivienda',\n",
        "    'bancofalab citisumma': 'Banco Falabella',\n",
        "    'exito': 'Tuya',\n",
        "    '√©xito': 'Tuya',\n",
        "    'Alkosto': 'Tuya',\n",
        "    'Corbeta': 'Tuya',\n",
        "    'Rappipay': 'Rappi',\n",
        "    'Rappicard': 'Rappi',\n",
        "    'Lulo Banck': 'Lulo Bank',\n",
        "    'LuloBank': 'Lulo Bank',\n",
        "    'QNT GIROS&FINANZAS': 'Banco Union',\n",
        "    'Codensa': 'Scotiabank Colpatria'\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# ==============================\n",
        "# 4. Funci√≥n de limpieza\n",
        "# ==============================\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"\n",
        "    Limpia y normaliza el texto: min√∫sculas, elimina caracteres no alfab√©ticos\n",
        "    (excepto acentos y √±/√º), elimina palabras irrelevantes y quita espacios dobles.\n",
        "    \"\"\"\n",
        "    texto = str(texto).lower()\n",
        "\n",
        "    # Dejar solo letras, acentos, √±, √º y espacios\n",
        "    texto = re.sub(r'[^a-z√°√©√≠√≥√∫√±√º\\s]', ' ', texto)\n",
        "\n",
        "    # Eliminar palabras \"de relleno\" frecuentes en reparadoras / BPO\n",
        "    texto = re.sub(\n",
        "        r'\\b('\n",
        "        r'grupo|juridico|jur√≠dico|sas|sa|s a|ltda|suma|financiera|'\n",
        "        r'contactosol|contacto|solucion|soluciones|citisumma|'\n",
        "        r'cobrando|cobranzas|adcore|logros|factoring|origen|origem|'\n",
        "        r'gestiones|gestion|profesionales|bpo|inversionistas|'\n",
        "        r'estrategicos|estrat√©gicos|casa|de|cobro|servicios|'\n",
        "        r'creditos|credito|abogados|asociados|'\n",
        "        r'outsourcing|risk|patrimonio|autonomo|aut√≥nomo|central|'\n",
        "        r'inversiones|valora|punto|com|puntocom|activos|'\n",
        "        r'recuperacion|recuperaci√≥n|financiera|financiero|'\n",
        "        r'asesores|asociados|gest|prof|eyc|gca|summa'\n",
        "        r')\\b',\n",
        "        '',\n",
        "        texto\n",
        "    )\n",
        "\n",
        "    # Espacios m√∫ltiples -> uno solo\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "    return texto\n",
        "\n",
        "# ==============================\n",
        "# 5. Claves de referencia para fuzzy\n",
        "# ==============================\n",
        "claves_referencia = {\n",
        "    (n.lower() if n in bancos_excepciones else re.sub(r'^banco\\s*', '', n.lower())): n\n",
        "    for n in nombres_correctos\n",
        "}\n",
        "\n",
        "# ==============================\n",
        "# 6. Mapeo por patr√≥n (texto original)\n",
        "# ==============================\n",
        "def mapear_por_patron(nombre_incorrecto):\n",
        "    \"\"\"\n",
        "    Intenta identificar el banco a partir del texto ORIGINAL (sin limpiar),\n",
        "    buscando los patrones definidos en PATRONES_POR_BANCO.\n",
        "    \"\"\"\n",
        "    if pd.isna(nombre_incorrecto):\n",
        "        return None\n",
        "\n",
        "    texto = str(nombre_incorrecto).lower()\n",
        "\n",
        "    for banco_estandar, patrones in PATRONES_POR_BANCO.items():\n",
        "        for patron in patrones:\n",
        "            if patron in texto:\n",
        "                return banco_estandar\n",
        "\n",
        "    return None\n",
        "\n",
        "# ==============================\n",
        "# 7. Funci√≥n principal de correcci√≥n\n",
        "# ==============================\n",
        "def corregir_nombre(nombre_incorrecto):\n",
        "    \"\"\"\n",
        "    Corrige un nombre de banco usando 3 capas:\n",
        "    1. Mapeo por patr√≥n sobre el texto original.\n",
        "    2. Limpieza de texto + alias manuales.\n",
        "    3. Fuzzy matching contra claves_referencia.\n",
        "    \"\"\"\n",
        "    # Si es NaN/None, lo dejamos igual\n",
        "    if pd.isna(nombre_incorrecto):\n",
        "        return nombre_incorrecto\n",
        "\n",
        "    # 1. Intentar primero con patrones evidentes en el texto original\n",
        "    banco_patron = mapear_por_patron(nombre_incorrecto)\n",
        "    if banco_patron is not None:\n",
        "        return banco_patron\n",
        "\n",
        "    # 2. Limpiar texto\n",
        "    limpio = limpiar_texto(nombre_incorrecto)\n",
        "\n",
        "    # Si despu√©s de limpiar no queda nada, devolvemos el original\n",
        "    if limpio == '':\n",
        "        return nombre_incorrecto\n",
        "\n",
        "    # 3. Alias manuales sobre texto limpio\n",
        "    for alias, banco_estandar in alias_manuales.items():\n",
        "        if alias in limpio:\n",
        "            return banco_estandar\n",
        "\n",
        "    # 4. Fuzzy matching con RapidFuzz\n",
        "    mejor_match, score, _ = process.extractOne(\n",
        "        limpio,\n",
        "        claves_referencia.keys(),\n",
        "        scorer=fuzz.token_set_ratio\n",
        "    )\n",
        "\n",
        "    # Si la similitud es alta, usamos el banco est√°ndar; si no, dejamos el texto original\n",
        "    return claves_referencia[mejor_match] if score > 70 else nombre_incorrecto\n",
        "\n",
        "# ==============================\n",
        "# 8. Aplicar al DataFrame\n",
        "# ==============================\n",
        "\n",
        "# Crear nueva columna estandarizada en reparadoras_df\n",
        "df_liq['BANCOS_ESTANDAR'] = df_liq['Banco'].apply(corregir_nombre)"
      ],
      "metadata": {
        "id": "8iQrJOmNgYYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ ZoneInfo compatible con Colab y GitHub (Py < 3.9)\n",
        "try:\n",
        "    from zoneinfo import ZoneInfo\n",
        "except ImportError:  # pragma: no cover\n",
        "    from backports.zoneinfo import ZoneInfo  # type: ignore\n",
        "\n",
        "\n",
        "def _to_float_money(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    s = str(x).strip()\n",
        "    if s == \"\":\n",
        "        return np.nan\n",
        "\n",
        "    s = s.replace(\"$\", \"\").replace(\"COP\", \"\").replace(\"cop\", \"\").replace(\" \", \"\")\n",
        "\n",
        "    if \",\" in s and \".\" in s:\n",
        "        if s.rfind(\",\") > s.rfind(\".\"):\n",
        "            s = s.replace(\".\", \"\")\n",
        "            s = s.replace(\",\", \".\")\n",
        "        else:\n",
        "            s = s.replace(\",\", \"\")\n",
        "    else:\n",
        "        if \",\" in s:\n",
        "            if s.count(\",\") > 1:\n",
        "                s = s.replace(\",\", \"\")\n",
        "            else:\n",
        "                tail = s.split(\",\")[-1]\n",
        "                s = s.replace(\",\", \".\") if len(tail) in (1, 2) else s.replace(\",\", \"\")\n",
        "        if \".\" in s:\n",
        "            if s.count(\".\") > 1:\n",
        "                s = s.replace(\".\", \"\")\n",
        "            else:\n",
        "                tail = s.split(\".\")[-1]\n",
        "                if len(tail) not in (1, 2):\n",
        "                    s = s.replace(\".\", \"\")\n",
        "\n",
        "    try:\n",
        "        return float(s)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def _to_bogota_datetime_ddmmyyyy(series, tz=\"America/Bogota\"):\n",
        "    tzinfo = ZoneInfo(tz)\n",
        "    dt = pd.to_datetime(series, errors=\"coerce\", dayfirst=True)\n",
        "    if getattr(dt.dt, \"tz\", None) is None:\n",
        "        dt = dt.dt.tz_localize(tzinfo, nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
        "    else:\n",
        "        dt = dt.dt.tz_convert(tzinfo)\n",
        "    return dt\n",
        "\n",
        "\n",
        "def _modo(s: pd.Series):\n",
        "    s = s.dropna().astype(str)\n",
        "    if s.empty:\n",
        "        return np.nan\n",
        "    return s.value_counts().index[0]\n",
        "\n",
        "\n",
        "def agregar_liquidaciones_al_timeline_con_fallback(\n",
        "    df_timeline: pd.DataFrame,\n",
        "    df_liq: pd.DataFrame,\n",
        ") -> pd.DataFrame:\n",
        "    df_tl = df_timeline.copy()\n",
        "    df_l = df_liq.copy()\n",
        "\n",
        "    # asegurar columnas nuevas\n",
        "    for c in [\"Negociador liquidacion\", \"Por?\"]:\n",
        "        if c not in df_tl.columns:\n",
        "            df_tl[c] = np.nan\n",
        "\n",
        "    # normalizar llaves\n",
        "    df_l[\"Id deuda\"] = pd.to_numeric(df_l[\"Deuda Berex\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    df_l[\"Referencia\"] = pd.to_numeric(df_l[\"Referencia\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    # fecha dd/mm/yyyy -> datetime Bogot√°\n",
        "    df_l[\"inserted_at\"] = _to_bogota_datetime_ddmmyyyy(df_l[\"Fecha de Liquidaci√≥n\"])\n",
        "\n",
        "    # pago a banco float\n",
        "    df_l[\"payment_to_bank\"] = df_l[\"Pago a banco\"].apply(_to_float_money).astype(float)\n",
        "\n",
        "    # mapas desde timeline para fallback  ‚úÖ (aqu√≠ solo a√±adimos tus 4 columnas)\n",
        "    base_by_debt = (\n",
        "        df_tl[\n",
        "            [\n",
        "                \"Id deuda\",\n",
        "                \"Referencia\",\n",
        "                \"BANCOS_ESTANDAR\",\n",
        "                \"Descuento\",\n",
        "                \"D_BRAVO\",\n",
        "                \"Tipo de Liquidacion\",\n",
        "                \"Ahorro total\",\n",
        "                \"Ahorro medio\",\n",
        "                \"Potencial\",\n",
        "                \"Potencial Credito\",\n",
        "            ]\n",
        "        ]\n",
        "        .dropna(subset=[\"Id deuda\"])\n",
        "        .drop_duplicates(subset=[\"Id deuda\"])\n",
        "        .set_index(\"Id deuda\")\n",
        "    )\n",
        "\n",
        "    map_ref_by_debt = base_by_debt[\"Referencia\"].to_dict()\n",
        "    map_banco_by_debt = base_by_debt[\"BANCOS_ESTANDAR\"].to_dict()\n",
        "    map_desc_by_debt = base_by_debt[\"Descuento\"].to_dict()\n",
        "    map_deuda_by_debt = base_by_debt[\"D_BRAVO\"].to_dict()\n",
        "    map_tipol_by_debt = base_by_debt[\"Tipo de Liquidacion\"].to_dict()\n",
        "\n",
        "    # ‚úÖ nuevos mapas\n",
        "    map_ahorro_total_by_debt = base_by_debt[\"Ahorro total\"].to_dict()\n",
        "    map_ahorro_medio_by_debt = base_by_debt[\"Ahorro medio\"].to_dict()\n",
        "    map_potencial_by_debt = base_by_debt[\"Potencial\"].to_dict()\n",
        "    map_pot_credito_by_debt = base_by_debt[\"Potencial Credito\"].to_dict()\n",
        "\n",
        "    # mapa negociador por referencia\n",
        "    map_neg_ref = (\n",
        "        df_tl[[\"Referencia\", \"Negociador\"]]\n",
        "        .dropna(subset=[\"Referencia\", \"Negociador\"])\n",
        "        .groupby(\"Referencia\")[\"Negociador\"]\n",
        "        .apply(_modo)\n",
        "        .to_dict()\n",
        "    )\n",
        "\n",
        "    banco_liq = df_l[\"BANCO_ESTANDAR\"] if \"BANCO_ESTANDAR\" in df_l.columns else pd.Series(np.nan, index=df_l.index)\n",
        "    deuda_res_liq = df_l[\"D_BRAVO\"] if \"D_BRAVO\" in df_l.columns else pd.Series(np.nan, index=df_l.index)\n",
        "\n",
        "    if \"descuento\" in df_l.columns:\n",
        "        desc_liq = pd.to_numeric(df_l[\"descuento\"], errors=\"coerce\")\n",
        "    else:\n",
        "        desc_liq = pd.Series(np.nan, index=df_l.index)\n",
        "\n",
        "    tipo_liq_liq = df_l[\"Tipo de liquidacion\"].astype(object)\n",
        "\n",
        "    filas_liq = pd.DataFrame({\n",
        "        \"Referencia\": df_l[\"Referencia\"],\n",
        "        \"Id deuda\": df_l[\"Id deuda\"],\n",
        "        \"Negociador\": np.nan,\n",
        "        \"BANCOS_ESTANDAR\": banco_liq.astype(object),\n",
        "        \"Descuento\": desc_liq.astype(float),\n",
        "        \"D_BRAVO\": deuda_res_liq,\n",
        "        \"Tipo de Liquidacion\": tipo_liq_liq,\n",
        "\n",
        "        # ‚úÖ nuevas columnas en la fila Liquidado (arrancan nulas)\n",
        "        \"Ahorro total\": np.nan,\n",
        "        \"Ahorro medio\": np.nan,\n",
        "        \"Potencial\": np.nan,\n",
        "        \"Potencial Credito\": np.nan,\n",
        "\n",
        "        \"inserted_at\": df_l[\"inserted_at\"],\n",
        "        \"end\": np.nan,\n",
        "        \"payment_to_bank\": df_l[\"payment_to_bank\"],\n",
        "        \"CATEGORIA_PRED\": \"Liquidado\",\n",
        "        \"observations\": np.nan,\n",
        "        \"tipo_fila\": \"Liquidaci√≥n\",\n",
        "        \"Negociador liquidacion\": df_l[\"Negociador\"].astype(str),\n",
        "        \"Por?\": df_l[\"Tipo de liquidacion\"].astype(str),\n",
        "    })\n",
        "\n",
        "    filas_liq = (\n",
        "        filas_liq.dropna(subset=[\"Id deuda\"])\n",
        "        .sort_values([\"Id deuda\", \"inserted_at\"])\n",
        "        .groupby(\"Id deuda\", as_index=False)\n",
        "        .tail(1)\n",
        "    )\n",
        "\n",
        "    filas_liq[\"Referencia\"] = filas_liq[\"Referencia\"].fillna(filas_liq[\"Id deuda\"].map(map_ref_by_debt))\n",
        "    filas_liq[\"Negociador\"] = filas_liq[\"Referencia\"].map(map_neg_ref)\n",
        "\n",
        "    filas_liq[\"BANCOS_ESTANDAR\"] = filas_liq[\"BANCOS_ESTANDAR\"].fillna(filas_liq[\"Id deuda\"].map(map_banco_by_debt))\n",
        "    filas_liq[\"Descuento\"] = filas_liq[\"Descuento\"].fillna(filas_liq[\"Id deuda\"].map(map_desc_by_debt))\n",
        "\n",
        "    filas_liq[\"D_BRAVO\"] = pd.to_numeric(filas_liq[\"D_BRAVO\"], errors=\"coerce\")\n",
        "    filas_liq[\"D_BRAVO\"] = filas_liq[\"D_BRAVO\"].fillna(filas_liq[\"Id deuda\"].map(map_deuda_by_debt))\n",
        "\n",
        "    filas_liq[\"Tipo de Liquidacion\"] = filas_liq[\"Tipo de Liquidacion\"].replace(\"\", np.nan)\n",
        "    filas_liq[\"Tipo de Liquidacion\"] = filas_liq[\"Tipo de Liquidacion\"].fillna(\n",
        "        filas_liq[\"Id deuda\"].map(map_tipol_by_debt)\n",
        "    )\n",
        "\n",
        "    # ‚úÖ fallback nuevo: igualito a lo dem√°s\n",
        "    filas_liq[\"Ahorro total\"] = filas_liq[\"Ahorro total\"].fillna(filas_liq[\"Id deuda\"].map(map_ahorro_total_by_debt))\n",
        "    filas_liq[\"Ahorro medio\"] = filas_liq[\"Ahorro medio\"].fillna(filas_liq[\"Id deuda\"].map(map_ahorro_medio_by_debt))\n",
        "    filas_liq[\"Potencial\"] = filas_liq[\"Potencial\"].fillna(filas_liq[\"Id deuda\"].map(map_potencial_by_debt))\n",
        "    filas_liq[\"Potencial Credito\"] = filas_liq[\"Potencial Credito\"].fillna(\n",
        "        filas_liq[\"Id deuda\"].map(map_pot_credito_by_debt)\n",
        "    )\n",
        "\n",
        "    filas_liq[\"Referencia\"] = filas_liq[\"Referencia\"].astype(\"Int64\")\n",
        "    filas_liq[\"Id deuda\"] = filas_liq[\"Id deuda\"].astype(\"Int64\")\n",
        "    filas_liq[\"Descuento\"] = pd.to_numeric(filas_liq[\"Descuento\"], errors=\"coerce\").astype(float)\n",
        "    filas_liq[\"D_BRAVO\"] = pd.to_numeric(filas_liq[\"D_BRAVO\"], errors=\"coerce\").astype(float)\n",
        "    filas_liq[\"payment_to_bank\"] = pd.to_numeric(filas_liq[\"payment_to_bank\"], errors=\"coerce\").astype(float)\n",
        "    filas_liq[\"inserted_at\"] = _to_bogota_datetime_ddmmyyyy(filas_liq[\"inserted_at\"])\n",
        "\n",
        "    for c in df_tl.columns:\n",
        "        if c not in filas_liq.columns:\n",
        "            filas_liq[c] = np.nan\n",
        "    for c in filas_liq.columns:\n",
        "        if c not in df_tl.columns:\n",
        "            df_tl[c] = np.nan\n",
        "\n",
        "    filas_liq = filas_liq[df_tl.columns]\n",
        "\n",
        "    out = (\n",
        "        pd.concat([df_tl, filas_liq], ignore_index=True)\n",
        "        .sort_values([\"Id deuda\", \"inserted_at\"], na_position=\"first\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ===== USO =====\n",
        "df_timeline_final = agregar_liquidaciones_al_timeline_con_fallback(df_timeline, df_liq)\n",
        "df_timeline_final.info()"
      ],
      "metadata": {
        "id": "l0x4IbnxgdhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df_timeline = df_timeline_final.copy()\n",
        "\n",
        "# -------------------------\n",
        "# 1) Pago banco esperado\n",
        "# -------------------------\n",
        "df_timeline[\"Pago_banco_esperado\"] = (\n",
        "    df_timeline[\"D_BRAVO\"]\n",
        "    - (df_timeline[\"D_BRAVO\"] * df_timeline[\"Descuento\"])\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 2) Ingreso esperado (CE = 0.15)\n",
        "# -------------------------\n",
        "CE = 0.15\n",
        "\n",
        "df_timeline[\"Ingreso_esperado\"] = np.maximum(\n",
        "    0,\n",
        "    (df_timeline[\"D_BRAVO\"] - df_timeline[\"Pago_banco_esperado\"]) * 1.19 * CE\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 3) Mover columna al lado de D_BRAVO\n",
        "# -------------------------\n",
        "cols = df_timeline.columns.tolist()\n",
        "idx = cols.index(\"D_BRAVO\")\n",
        "\n",
        "# quitamos y reinsertamos\n",
        "cols.remove(\"Ingreso_esperado\")\n",
        "cols.insert(idx + 1, \"Ingreso_esperado\")\n",
        "\n",
        "df_timeline = df_timeline[cols]\n",
        "\n",
        "df_timeline[[\n",
        "    \"D_BRAVO\",\n",
        "    \"Descuento\",\n",
        "    \"Pago_banco_esperado\",\n",
        "    \"Ingreso_esperado\"\n",
        "]].head()"
      ],
      "metadata": {
        "id": "zS-J7hoQKZ0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline"
      ],
      "metadata": {
        "id": "ay4izxXQKbcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "df_liq_aux = df_liq.copy()\n",
        "\n",
        "def _norm_col(x: str) -> str:\n",
        "    # normaliza: lower, quita tildes, colapsa espacios\n",
        "    x = str(x).replace(\"\\u00a0\", \" \")\n",
        "    x = re.sub(r\"\\s+\", \" \", x).strip().lower()\n",
        "    x = \"\".join(ch for ch in unicodedata.normalize(\"NFD\", x) if unicodedata.category(ch) != \"Mn\")\n",
        "    return x\n",
        "\n",
        "# --- 1) Encontrar el nombre REAL de columnas (robusto)\n",
        "cols_norm = {c: _norm_col(c) for c in df_liq_aux.columns}\n",
        "\n",
        "# deuda (lo que t√∫ quieres para D_BRAVO) = \"Deuda Resuelve\"\n",
        "col_deuda = next((c for c, cn in cols_norm.items() if cn in (\"d_bravo\", \"deuda resuelve\", \"deuda_resuelve\")), None)\n",
        "\n",
        "# banco\n",
        "col_banco = next((c for c, cn in cols_norm.items() if cn in (\"bancos_estandar\", \"banco estandar\", \"banco_estandar\")), None)\n",
        "\n",
        "# id berex\n",
        "col_berex = next((c for c, cn in cols_norm.items() if cn in (\"deuda berex\", \"deuda_berex\", \"id deuda berex\", \"id_deuda_berex\")), None)\n",
        "\n",
        "print(\"Columna deuda detectada:\", col_deuda)\n",
        "print(\"Columna banco detectada:\", col_banco)\n",
        "print(\"Columna berex detectada:\", col_berex)\n",
        "\n",
        "if col_deuda is None:\n",
        "    raise KeyError(\"No encontr√© la columna de deuda (esperaba 'Deuda Resuelve' o 'D_BRAVO').\")\n",
        "if col_banco is None:\n",
        "    raise KeyError(\"No encontr√© la columna 'BANCOS_ESTANDAR' (ni variantes).\")\n",
        "if col_berex is None:\n",
        "    raise KeyError(\"No encontr√© la columna 'Deuda Berex' (ni variantes).\")\n",
        "\n",
        "# --- 2) Construir Id deuda desde Deuda Berex\n",
        "df_liq_aux[\"Id deuda\"] = pd.to_numeric(df_liq_aux[col_berex], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# --- 3) Parse D_BRAVO a float (robusto)\n",
        "s = df_liq_aux[col_deuda].astype(str).str.strip()\n",
        "\n",
        "# limpiar s√≠mbolos y separar miles/decimales t√≠pico de COP\n",
        "s = s.str.replace(r\"[^\\d,\\.]\", \"\", regex=True)\n",
        "\n",
        "# caso t√≠pico: \"45.488.000\" -> quitar puntos miles\n",
        "# y \"45,488,000\" -> quitar comas miles\n",
        "# y si viene con decimal, lo intentamos preservar\n",
        "# estrategia: si tiene ambos, asumimos decimal el √∫ltimo separador\n",
        "def _parse_money_str(x):\n",
        "    if x in (\"\", \"nan\", \"None\"):\n",
        "        return np.nan\n",
        "    if \",\" in x and \".\" in x:\n",
        "        # el separador que aparezca m√°s a la derecha es decimal\n",
        "        if x.rfind(\",\") > x.rfind(\".\"):\n",
        "            x = x.replace(\".\", \"\")\n",
        "            x = x.replace(\",\", \".\")\n",
        "        else:\n",
        "            x = x.replace(\",\", \"\")\n",
        "    else:\n",
        "        # si solo tiene comas: puede ser miles o decimal\n",
        "        if \",\" in x:\n",
        "            parts = x.split(\",\")\n",
        "            # si √∫ltimo bloque tiene 1-2 d√≠gitos, decimal\n",
        "            if len(parts[-1]) in (1,2):\n",
        "                x = x.replace(\".\", \"\")\n",
        "                x = x.replace(\",\", \".\")\n",
        "            else:\n",
        "                x = x.replace(\",\", \"\")\n",
        "        # si solo tiene puntos: miles o decimal\n",
        "        if \".\" in x:\n",
        "            parts = x.split(\".\")\n",
        "            if len(parts[-1]) not in (1,2):  # no parece decimal\n",
        "                x = x.replace(\".\", \"\")\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df_liq_aux[\"Deuda_Resuelve_num\"] = s.map(_parse_money_str)\n",
        "\n",
        "# --- 4) Mapas por Id deuda\n",
        "map_banco_liq = (\n",
        "    df_liq_aux.dropna(subset=[\"Id deuda\", col_banco])\n",
        "             .drop_duplicates(subset=[\"Id deuda\"])\n",
        "             .set_index(\"Id deuda\")[col_banco]\n",
        ")\n",
        "\n",
        "map_deuda_liq = (\n",
        "    df_liq_aux.dropna(subset=[\"Id deuda\", \"Deuda_Resuelve_num\"])\n",
        "             .drop_duplicates(subset=[\"Id deuda\"])\n",
        "             .set_index(\"Id deuda\")[\"Deuda_Resuelve_num\"]\n",
        ")\n",
        "\n",
        "# --- 5) Rellenar SOLO filas Liquidaci√≥n y SOLO si est√° nulo\n",
        "df = df_timeline.copy()\n",
        "mask_liq = df[\"tipo_fila\"].astype(\"string\").eq(\"Liquidaci√≥n\")\n",
        "\n",
        "df.loc[mask_liq, \"BANCOS_ESTANDAR\"] = (\n",
        "    df.loc[mask_liq, \"BANCOS_ESTANDAR\"]\n",
        "      .fillna(df.loc[mask_liq, \"Id deuda\"].map(map_banco_liq))\n",
        ")\n",
        "\n",
        "df.loc[mask_liq, \"D_BRAVO\"] = (\n",
        "    df.loc[mask_liq, \"D_BRAVO\"]\n",
        "      .fillna(df.loc[mask_liq, \"Id deuda\"].map(map_deuda_liq))\n",
        ")\n",
        "\n",
        "df[\"D_BRAVO\"] = pd.to_numeric(df[\"D_BRAVO\"], errors=\"coerce\").astype(float)\n",
        "\n",
        "df_timeline_final = df\n",
        "\n",
        "# --- 6) Chequeo\n",
        "m = df_timeline_final[\"tipo_fila\"].astype(\"string\").eq(\"Liquidaci√≥n\")\n",
        "print(\"Liquidaci√≥n sin BANCOS_ESTANDAR:\", df_timeline_final.loc[m, \"BANCOS_ESTANDAR\"].isna().sum())\n",
        "print(\"Liquidaci√≥n sin D_BRAVO:\", df_timeline_final.loc[m, \"D_BRAVO\"].isna().sum())"
      ],
      "metadata": {
        "id": "klFhlqJ-Kqbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final"
      ],
      "metadata": {
        "id": "3vzJhz7GKv72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# BASE FUNNEL COMPLETA (1 fila por Id deuda) desde df_timeline_final\n",
        "# Compatible con Google Colab y GitHub / ejecuci√≥n local\n",
        "# =========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Copia segura\n",
        "# -----------------------------\n",
        "df = df_timeline_final.copy()\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Tipos / limpieza base\n",
        "# -----------------------------\n",
        "df[\"Id deuda\"] = pd.to_numeric(df.get(\"Id deuda\"), errors=\"coerce\").astype(\"Int64\")\n",
        "df[\"Referencia\"] = pd.to_numeric(df.get(\"Referencia\"), errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "df[\"inserted_at\"] = pd.to_datetime(df.get(\"inserted_at\"), errors=\"coerce\")\n",
        "\n",
        "df[\"D_BRAVO\"] = pd.to_numeric(df.get(\"D_BRAVO\"), errors=\"coerce\")\n",
        "df[\"Ingreso_esperado\"] = pd.to_numeric(df.get(\"Ingreso_esperado\"), errors=\"coerce\")\n",
        "df[\"payment_to_bank\"] = pd.to_numeric(df.get(\"payment_to_bank\"), errors=\"coerce\")\n",
        "\n",
        "if \"CATEGORIA_PRED\" not in df.columns:\n",
        "    raise ValueError(\"df_timeline_final no tiene la columna 'CATEGORIA_PRED'.\")\n",
        "\n",
        "df[\"_cat_all_norm\"] = (\n",
        "    df[\"CATEGORIA_PRED\"]\n",
        "    .astype(\"string\")\n",
        "    .str.strip()\n",
        "    .str.upper()\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Hist√≥rico liquidado\n",
        "# -----------------------------\n",
        "liq_hist = (\n",
        "    df.groupby(\"Id deuda\")[\"_cat_all_norm\"]\n",
        "      .apply(lambda s: (s == \"LIQUIDADO\").any())\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 3) √öltimo registro por deuda\n",
        "# -----------------------------\n",
        "df = df.sort_values([\"Id deuda\", \"inserted_at\"], na_position=\"last\")\n",
        "df_ult = df.groupby(\"Id deuda\", as_index=False).tail(1).copy()\n",
        "\n",
        "df_ult = df_ult.rename(columns={\n",
        "    \"inserted_at\": \"inserted_at_ultima\",\n",
        "    \"observations\": \"observations_ultima\",\n",
        "    \"CATEGORIA_PRED\": \"CATEGORIA_PRED_ultima\",\n",
        "    \"payment_to_bank\": \"payment_to_bank_ultima\",\n",
        "    \"end\": \"end_ultima\",\n",
        "})\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Flags observaci√≥n / mes actual (Bogot√°)\n",
        "# -----------------------------\n",
        "obs = (\n",
        "    df_ult.get(\"observations_ultima\", pd.Series(pd.NA, index=df_ult.index))\n",
        "    .astype(\"string\")\n",
        "    .str.strip()\n",
        ")\n",
        "df_ult[\"tiene_obs\"] = obs.notna() & (obs != \"\")\n",
        "\n",
        "hoy = pd.Timestamp.now(tz=\"America/Bogota\")\n",
        "inicio_mes = hoy.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
        "\n",
        "col = \"inserted_at_ultima\"\n",
        "df_ult[col] = pd.to_datetime(df_ult[col], errors=\"coerce\")\n",
        "\n",
        "if df_ult[col].dt.tz is None:\n",
        "    df_ult[col] = df_ult[col].dt.tz_localize(\n",
        "        \"America/Bogota\",\n",
        "        nonexistent=\"shift_forward\",\n",
        "        ambiguous=\"NaT\"\n",
        "    )\n",
        "else:\n",
        "    df_ult[col] = df_ult[col].dt.tz_convert(\"America/Bogota\")\n",
        "\n",
        "df_ult[\"es_este_mes\"] = df_ult[col].notna() & (df_ult[col] >= inicio_mes)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Normalizar categor√≠a √∫ltima\n",
        "# -----------------------------\n",
        "df_ult[\"cat_norm\"] = (\n",
        "    df_ult[\"CATEGORIA_PRED_ultima\"]\n",
        "    .astype(\"string\")\n",
        "    .str.strip()\n",
        "    .str.upper()\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Liquidado definitivo\n",
        "# -----------------------------\n",
        "df_ult[\"tiene_liquidado_historico\"] = df_ult[\"Id deuda\"].map(liq_hist).fillna(False)\n",
        "cond_f4 = df_ult[\"tiene_liquidado_historico\"]\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Fase 3 ‚Äî Avance\n",
        "# -----------------------------\n",
        "es_avance_cat = df_ult[\"cat_norm\"].isin(\n",
        "    [\"ACUERDO\", \"DESCUENTO\", \"CONTRAPROPUESTA\"]\n",
        ").fillna(False)\n",
        "\n",
        "deuda = pd.to_numeric(df_ult[\"D_BRAVO\"], errors=\"coerce\")\n",
        "pab = pd.to_numeric(df_ult[\"payment_to_bank_ultima\"], errors=\"coerce\")\n",
        "cond_f3 = (es_avance_cat & ((deuda - pab) >= 10000)).fillna(False)\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Fase 1 ‚Äî Sin actualizar\n",
        "# -----------------------------\n",
        "cond_f1 = ((~df_ult[\"tiene_obs\"]) | (~df_ult[\"es_este_mes\"])).fillna(False)\n",
        "\n",
        "# -----------------------------\n",
        "# 9) Fase 2 ‚Äî Actualizado\n",
        "# -----------------------------\n",
        "cond_f2 = ((~cond_f1) & (~cond_f3) & (~cond_f4)).fillna(False)\n",
        "\n",
        "# -----------------------------\n",
        "# 10) Asignar FASE\n",
        "# -----------------------------\n",
        "df_ult[\"FASE\"] = np.select(\n",
        "    [\n",
        "        cond_f4.to_numpy(bool),\n",
        "        cond_f3.to_numpy(bool),\n",
        "        cond_f2.to_numpy(bool),\n",
        "        cond_f1.to_numpy(bool),\n",
        "    ],\n",
        "    [\n",
        "        \"Fase 4 ‚Äî Liquidado\",\n",
        "        \"Fase 3 ‚Äî Avance\",\n",
        "        \"Fase 2 ‚Äî Actualizado\",\n",
        "        \"Fase 1 ‚Äî Sin actualizar / antes de mes\",\n",
        "    ],\n",
        "    default=\"Fase 2 ‚Äî Actualizado\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 11) STATUS\n",
        "# -----------------------------\n",
        "status_cat = df_ult[\"cat_norm\"].str.title()\n",
        "\n",
        "df_ult[\"STATUS\"] = np.select(\n",
        "    [\n",
        "        cond_f4.to_numpy(bool),\n",
        "        cond_f3.to_numpy(bool),\n",
        "        cond_f2.to_numpy(bool),\n",
        "        (~df_ult[\"tiene_obs\"]).to_numpy(bool),\n",
        "        (df_ult[\"tiene_obs\"] & ~df_ult[\"es_este_mes\"]).to_numpy(bool),\n",
        "    ],\n",
        "    [\n",
        "        \"Liquidado\",\n",
        "        status_cat,\n",
        "        status_cat,\n",
        "        \"Sin actualizaci√≥n\",\n",
        "        \"Actualizado antes\",\n",
        "    ],\n",
        "    default=status_cat\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 12) Ingreso funnel\n",
        "# -----------------------------\n",
        "df_ult[\"Ingreso_funnel\"] = np.where(\n",
        "    cond_f4.to_numpy(bool),\n",
        "    0,\n",
        "    df_ult[\"Ingreso_esperado\"].fillna(0)\n",
        ")\n",
        "# -----------------------------\n",
        "# 13) Base final (con columnas requeridas)\n",
        "# -----------------------------\n",
        "cols_requeridas = [\n",
        "    \"Referencia\",\n",
        "    \"Id deuda\",\n",
        "    \"Cedula\",\n",
        "    \"Nombre del cliente\",\n",
        "    \"Negociador\",\n",
        "    \"BANCOS_ESTANDAR\",\n",
        "    \"Descuento\",\n",
        "    \"D_BRAVO\",\n",
        "    \"MORA\"\n",
        "    \"Estructurable\",\n",
        "    \"Potencial\",\n",
        "    \"Meses en el Programa\",\n",
        "    \"Tipo de Liquidacion\",\n",
        "    \"Bucket\",\n",
        "    \"Ahorro total\",\n",
        "    \"Ahorro medio\",\n",
        "    \"Por cobrar\",\n",
        "    \"Potencial Credito\",\n",
        "    \"Estado Deuda\",\n",
        "    \"sub_estado_deuda\",\n",
        "    \"estado_reparadora\",\n",
        "    \"sub_estado_reparadora\",\n",
        "    \"Mora_estructurado\",\n",
        "    \"MORA_CREDITO\",\n",
        "    \"Priority_level\",\n",
        "    \"Ingreso_esperado\",\n",
        "    \"Ultimo contacto\",\n",
        "    \"fecha mensaje\"\n",
        "]\n",
        "\n",
        "cols_eventos_ultima = [\n",
        "    \"inserted_at_ultima\",\n",
        "    \"end_ultima\",\n",
        "    \"CATEGORIA_PRED_ultima\",\n",
        "    \"payment_to_bank_ultima\",\n",
        "    \"observations_ultima\",\n",
        "]\n",
        "\n",
        "cols_funnel = [\n",
        "    \"Ingreso_funnel\",\n",
        "    \"FASE\",\n",
        "    \"STATUS\",\n",
        "    \"tiene_obs\",\n",
        "    \"es_este_mes\",\n",
        "    \"tiene_liquidado_historico\",\n",
        "]\n",
        "\n",
        "# 1) Garantiza que existan (si no, NA)\n",
        "for c in (cols_requeridas + cols_eventos_ultima + cols_funnel):\n",
        "    if c not in df_ult.columns:\n",
        "        df_ult[c] = pd.NA\n",
        "\n",
        "# 2) Orden: primero requeridas, luego eventos, luego funnel, luego cualquier otra que exista\n",
        "extras = [c for c in df_ult.columns if c not in (cols_requeridas + cols_eventos_ultima + cols_funnel)]\n",
        "cols_salida = cols_requeridas + cols_eventos_ultima + cols_funnel + extras\n",
        "\n",
        "df_base_funnel = df_ult[cols_salida].copy()\n",
        "\n",
        "print(\"‚úÖ df_base_funnel listo\")\n",
        "print(\"shape:\", df_base_funnel.shape)\n",
        "print(\"\\nDistribuci√≥n por FASE:\")\n",
        "print(df_base_funnel[\"FASE\"].value_counts(dropna=False))\n",
        "\n",
        "print(\"\\nLiquidado (Fase 4) - filas:\",\n",
        "      (df_base_funnel[\"FASE\"] == \"Fase 4 ‚Äî Liquidado\").sum())\n",
        "\n",
        "print(df_base_funnel.head(20))"
      ],
      "metadata": {
        "id": "Ye5V0r-1K0lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final"
      ],
      "metadata": {
        "id": "Bnu9hRr-LIgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_base_funnel.info()"
      ],
      "metadata": {
        "id": "C9X7ROMuLP7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final.info()"
      ],
      "metadata": {
        "id": "lKLfiHuLLRxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Google Sheets en Colab + GitHub usando MI_JSON\n",
        "# - En Colab: lee MI_JSON desde Secrets (userdata)\n",
        "# - En GitHub / local: lee MI_JSON desde variable de entorno o archivo .json\n",
        "# =====================================================\n",
        "\n",
        "!pip install -q gspread gspread_dataframe\n",
        "\n",
        "import os, json\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from gspread_dataframe import get_as_dataframe\n",
        "\n",
        "def _load_service_account_info():\n",
        "    \"\"\"\n",
        "    Devuelve el dict del service account.\n",
        "    Prioridad:\n",
        "      1) Colab: userdata.get(\"MI_JSON\")  (tu regla)\n",
        "      2) Env var: MI_JSON (json string) o MI_JSON_PATH (ruta a .json)\n",
        "      3) Archivo: service_account.json en el repo (si existe)\n",
        "    \"\"\"\n",
        "    # 1) Colab (seguro si existe)\n",
        "    try:\n",
        "        from google.colab import userdata  # solo existe en Colab\n",
        "        mi_json = userdata.get(\"MI_JSON\")  # <-- tal como lo pediste\n",
        "        if mi_json:\n",
        "            return json.loads(mi_json) if isinstance(mi_json, str) else mi_json\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) GitHub/Local: env var MI_JSON (contenido json)\n",
        "    mi_json_env = os.getenv(\"MI_JSON\")\n",
        "    if mi_json_env:\n",
        "        return json.loads(mi_json_env)\n",
        "\n",
        "    # 2b) GitHub/Local: env var MI_JSON_PATH (ruta a archivo json)\n",
        "    mi_json_path = os.getenv(\"MI_JSON_PATH\")\n",
        "    if mi_json_path and os.path.exists(mi_json_path):\n",
        "        with open(mi_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    # 3) Fallback: archivo en el repo (NO recomendado si es privado, pero √∫til local)\n",
        "    default_path = \"service_account.json\"\n",
        "    if os.path.exists(default_path):\n",
        "        with open(default_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    raise RuntimeError(\n",
        "        \"No encontr√© credenciales. En Colab aseg√∫rate de tener el Secret MI_JSON. \"\n",
        "        \"En GitHub configura env var MI_JSON (json) o MI_JSON_PATH (ruta).\"\n",
        "    )\n",
        "\n",
        "def get_gspread_client():\n",
        "    sa_info = _load_service_account_info()\n",
        "    return gspread.service_account_from_dict(sa_info)\n",
        "\n",
        "# =====================================================\n",
        "# Leer la hoja \"Asignacion\"\n",
        "# =====================================================\n",
        "SPREADSHEET_ID = \"184zZcHRajscoJRyjMkkQjPh0qOlY3iVkQICKHl-iOyU\"\n",
        "NOMBRE_HOJA = \"Asignacion\"\n",
        "\n",
        "gc = get_gspread_client()\n",
        "sh = gc.open_by_key(SPREADSHEET_ID)\n",
        "ws = sh.worksheet(NOMBRE_HOJA)\n",
        "\n",
        "df_asignacion = get_as_dataframe(ws, evaluate_formulas=True).dropna(how=\"all\")\n",
        "df_asignacion.columns = df_asignacion.columns.str.strip()\n",
        "\n",
        "df_asignacion"
      ],
      "metadata": {
        "id": "EQWg7YJVtV-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final.info()"
      ],
      "metadata": {
        "id": "9vYdOh7PSzcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqu√≠ empezamos con los Bucket"
      ],
      "metadata": {
        "id": "FqFlImSUS17i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =========================================================\n",
        "# 1) Descuento_Actualizacion\n",
        "#    Si payment_to_bank no es nulo: ((payment_to_bank/D_BRAVO)-1)*-1\n",
        "# =========================================================\n",
        "df_timeline_final[\"Descuento_Actualizacion\"] = np.where(\n",
        "    df_timeline_final[\"payment_to_bank\"].notna(),\n",
        "    ((df_timeline_final[\"payment_to_bank\"] / df_timeline_final[\"D_BRAVO\"]) - 1) * -1,\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 2) Tipo de Actividad (reglas)\n",
        "# =========================================================\n",
        "cat = df_timeline_final[\"CATEGORIA_PRED\"]\n",
        "desc = df_timeline_final[\"Descuento_Actualizacion\"]\n",
        "\n",
        "cats_efectiva = {\"ACUERDO\", \"DESCUENTO\", \"EN_CREDITO\", \"Liquidado\", \"SALDOS\", \"DEPOSITO\"}\n",
        "rango_desc = desc.between(0.03, 0.98, inclusive=\"both\")\n",
        "\n",
        "# EFECTIVA\n",
        "cond_efectiva = cat.isin(cats_efectiva) & rango_desc\n",
        "\n",
        "# ACTUALIZADO_ALIANZAS\n",
        "cond_act_alianzas = (cat == \"ACTUALIZADO_ALIANZAS\")\n",
        "\n",
        "# EFECTIVA DESCUENTO ALIANZAS:\n",
        "# payment_to_bank < D_BRAVO y (D_BRAVO - payment_to_bank) <= 5000\n",
        "diff = df_timeline_final[\"D_BRAVO\"] - df_timeline_final[\"payment_to_bank\"]\n",
        "cond_desc_alianzas = (\n",
        "    cond_act_alianzas &\n",
        "    (df_timeline_final[\"payment_to_bank\"] < df_timeline_final[\"D_BRAVO\"]) &\n",
        "    (diff <= 5000)\n",
        ")\n",
        "\n",
        "df_timeline_final[\"Tipo de Actividad\"] = np.select(\n",
        "    [cond_efectiva, cond_desc_alianzas, cond_act_alianzas],\n",
        "    [\"EFECTIVA\", \"EFECTIVA DESCUENTO ALIANZAS\", \"EFECTIVA ALIANZAS\"],\n",
        "    default=\"NO EFECTIVA\"\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 3) Si payment_to_bank, CATEGORIA_PRED y observations son NaN -> Tipo de Actividad = NaN\n",
        "# =========================================================\n",
        "cond_nan_total = (\n",
        "    df_timeline_final[\"payment_to_bank\"].isna() &\n",
        "    df_timeline_final[\"CATEGORIA_PRED\"].isna() &\n",
        "    df_timeline_final[\"observations\"].isna()\n",
        ")\n",
        "df_timeline_final.loc[cond_nan_total, \"Tipo de Actividad\"] = np.nan\n",
        "\n",
        "# =========================================================\n",
        "# 4) Insertar ambas columnas al lado de 'observations'\n",
        "#    (quedan inmediatamente despu√©s de observations, en este orden)\n",
        "# =========================================================\n",
        "idx_obs = df_timeline_final.columns.get_loc(\"observations\")\n",
        "\n",
        "# Pop + insert para asegurar ubicaci√≥n/orden\n",
        "col_da = df_timeline_final.pop(\"Descuento_Actualizacion\")\n",
        "col_ta = df_timeline_final.pop(\"Tipo de Actividad\")\n",
        "\n",
        "df_timeline_final.insert(idx_obs + 1, \"Descuento_Actualizacion\", col_da)\n",
        "df_timeline_final.insert(idx_obs + 2, \"Tipo de Actividad\", col_ta)"
      ],
      "metadata": {
        "id": "FtWZsVgcWveI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_base_funnel.columns"
      ],
      "metadata": {
        "id": "EPpnDQ3f_5vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =========================================================\n",
        "# 1) Descuento_Actualizacion\n",
        "#    Si payment_to_bank no es nulo: ((payment_to_bank/D_BRAVO)-1)*-1\n",
        "# =========================================================\n",
        "df_base_funnel[\"Descuento_Actualizacion\"] = np.where(\n",
        "    df_base_funnel[\"payment_to_bank_ultima\"].notna(),\n",
        "    ((df_base_funnel[\"payment_to_bank_ultima\"] / df_base_funnel[\"D_BRAVO\"]) - 1) * -1,\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 2) Tipo de Actividad (reglas)\n",
        "# =========================================================\n",
        "cat = df_base_funnel[\"CATEGORIA_PRED_ultima\"]\n",
        "desc = df_base_funnel[\"Descuento_Actualizacion\"]\n",
        "\n",
        "cats_efectiva = {\"ACUERDO\", \"DESCUENTO\", \"EN_CREDITO\", \"Liquidado\", \"SALDOS\", \"DEPOSITO\"}\n",
        "rango_desc = desc.between(0.03, 0.98, inclusive=\"both\")\n",
        "\n",
        "# EFECTIVA\n",
        "cond_efectiva = cat.isin(cats_efectiva) & rango_desc\n",
        "\n",
        "# ACTUALIZADO_ALIANZAS\n",
        "cond_act_alianzas = (cat == \"ACTUALIZADO_ALIANZAS\")\n",
        "\n",
        "# EFECTIVA DESCUENTO ALIANZAS:\n",
        "# payment_to_bank < D_BRAVO y (D_BRAVO - payment_to_bank) <= 5000\n",
        "diff = df_base_funnel[\"D_BRAVO\"] - df_base_funnel[\"payment_to_bank_ultima\"]\n",
        "cond_desc_alianzas = (\n",
        "    cond_act_alianzas &\n",
        "    (df_base_funnel[\"payment_to_bank_ultima\"] < df_base_funnel[\"D_BRAVO\"]) &\n",
        "    (diff <= 5000)\n",
        ")\n",
        "\n",
        "df_base_funnel[\"Tipo de Actividad\"] = np.select(\n",
        "    [cond_efectiva, cond_desc_alianzas, cond_act_alianzas],\n",
        "    [\"EFECTIVA\", \"EFECTIVA DESCUENTO ALIANZAS\", \"EFECTIVA ALIANZAS\"],\n",
        "    default=\"NO EFECTIVA\"\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 3) Si payment_to_bank, CATEGORIA_PRED y observations son NaN -> Tipo de Actividad = NaN\n",
        "# =========================================================\n",
        "cond_nan_total = (\n",
        "    df_base_funnel[\"payment_to_bank_ultima\"].isna() &\n",
        "    df_base_funnel[\"CATEGORIA_PRED_ultima\"].isna() &\n",
        "    df_base_funnel[\"observations_ultima\"].isna()\n",
        ")\n",
        "df_base_funnel.loc[cond_nan_total, \"Tipo de Actividad\"] = np.nan\n",
        "\n",
        "# =========================================================\n",
        "# 4) Insertar ambas columnas al lado de 'observations'\n",
        "#    (quedan inmediatamente despu√©s de observations, en este orden)\n",
        "# =========================================================\n",
        "idx_obs = df_base_funnel.columns.get_loc(\"observations_ultima\")\n",
        "\n",
        "# Pop + insert para asegurar ubicaci√≥n/orden\n",
        "col_da = df_base_funnel.pop(\"Descuento_Actualizacion\")\n",
        "col_ta = df_base_funnel.pop(\"Tipo de Actividad\")\n",
        "\n",
        "df_base_funnel.insert(idx_obs + 1, \"Descuento_Actualizacion\", col_da)\n",
        "df_base_funnel.insert(idx_obs + 2, \"Tipo de Actividad\", col_ta)"
      ],
      "metadata": {
        "id": "LqCGlTrL-o5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_base_funnel = df_base_funnel.drop(columns=[\"Bucket\"], errors=\"ignore\")"
      ],
      "metadata": {
        "id": "u4GE8YpW_UwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final = df_timeline_final.drop(columns=[\"Bucket\"], errors=\"ignore\")"
      ],
      "metadata": {
        "id": "Jh2HEXqtf-nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final.info()"
      ],
      "metadata": {
        "id": "2oGv4s4tfH2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final"
      ],
      "metadata": {
        "id": "evDJ6cP2lAbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =========================================================\n",
        "# CONFIG\n",
        "# =========================================================\n",
        "COL_ULT_CONTACTO = \"ultimo contacto\"   # <- as√≠, con espacio\n",
        "COL_NEGOCIADOR = \"Negociador\"          # <- AJUSTA si tu columna se llama distinto\n",
        "COL_BANCO = \"BANCOS_ESTANDAR\"\n",
        "\n",
        "# Distribuci√≥n objetivo por negociador\n",
        "TARGET_PCTS = {0: 0.25, 1: 0.20, 2: 0.25, 3: 0.10, 4: 0.10, 5: 0.10}\n",
        "BUCKETS = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "# Pesos para score final (puedes ajustarlos)\n",
        "W_BASE = 0.75   # peso de lo ya construido (Bucket_row consolidado)\n",
        "W_BANK = 0.25   # peso del banco\n",
        "# Ahorro/D_BRAVO entra como impulso suave por buckets (ver m√°s abajo)\n",
        "\n",
        "# M√°s peso al mejor banco que al peor (para el criterio banco)\n",
        "ALPHA_BEST_BANK = 0.70\n",
        "\n",
        "# =========================================================\n",
        "# 0) COPIA + LIMPIEZA / NORMALIZACI√ìN\n",
        "# =========================================================\n",
        "df_timeline_final = df_timeline_final.copy()\n",
        "\n",
        "# Num√©ricas\n",
        "df_timeline_final[COL_ULT_CONTACTO] = pd.to_numeric(df_timeline_final[COL_ULT_CONTACTO], errors=\"coerce\").fillna(6)\n",
        "df_timeline_final[\"Potencial\"] = pd.to_numeric(df_timeline_final[\"Potencial\"], errors=\"coerce\")\n",
        "df_timeline_final[\"Estructurable\"] = pd.to_numeric(df_timeline_final[\"Estructurable\"], errors=\"coerce\")\n",
        "df_timeline_final[\"MORA\"] = pd.to_numeric(df_timeline_final[\"MORA\"], errors=\"coerce\")\n",
        "df_timeline_final[\"Meses en el Programa\"] = pd.to_numeric(df_timeline_final[\"Meses en el Programa\"], errors=\"coerce\")\n",
        "df_timeline_final[\"D_BRAVO\"] = pd.to_numeric(df_timeline_final[\"D_BRAVO\"], errors=\"coerce\")\n",
        "df_timeline_final[\"Ahorro total\"] = pd.to_numeric(df_timeline_final[\"Ahorro total\"], errors=\"coerce\")\n",
        "\n",
        "# Textos\n",
        "mora_credito = df_timeline_final[\"MORA_CREDITO\"].astype(\"string\").str.strip().str.lower()\n",
        "mora_estruct = df_timeline_final[\"Mora_estructurado\"].astype(\"string\").str.strip().str.lower()\n",
        "cat_pred = df_timeline_final[\"CATEGORIA_PRED\"].astype(\"string\").str.strip()\n",
        "cat_upper = cat_pred.str.upper()\n",
        "\n",
        "# Potencial Credito mixto\n",
        "pc_raw = df_timeline_final[\"Potencial Credito\"].astype(\"string\").str.strip()\n",
        "pc_lower = pc_raw.str.lower()\n",
        "pc_num = pd.to_numeric(pc_raw, errors=\"coerce\")\n",
        "\n",
        "# =========================================================\n",
        "# 1) BUCKET \"POR FILA\" (PASO INTERMEDIO) => Bucket_row (0..5)\n",
        "# =========================================================\n",
        "# 1.1 Base por recencia (din√°mico 0..5)\n",
        "s = df_timeline_final[COL_ULT_CONTACTO]\n",
        "base = pd.qcut(s.rank(method=\"first\"), q=6, labels=False, duplicates=\"drop\")\n",
        "n_bins = int(base.max() + 1) if base.notna().any() else 1\n",
        "if n_bins > 1:\n",
        "    base_scaled = np.floor(base * (5 / (n_bins - 1))).astype(int)\n",
        "else:\n",
        "    base_scaled = pd.Series(0, index=df_timeline_final.index, dtype=int)\n",
        "df_timeline_final[\"Bucket_row\"] = base_scaled.astype(\"Int64\")\n",
        "\n",
        "# 1.2 Penalizaci√≥n por cliente con al menos 1 \"Liquidado\" (suave +1)\n",
        "#clientes_con_liquidado = df_timeline_final.loc[cat_pred.eq(\"Liquidado\"), \"Referencia\"].dropna().unique()\n",
        "#mask_cliente_liquidado = df_timeline_final[\"Referencia\"].isin(clientes_con_liquidado)\n",
        "#df_timeline_final.loc[mask_cliente_liquidado, \"Bucket_row\"] = (\n",
        "#    df_timeline_final.loc[mask_cliente_liquidado, \"Bucket_row\"].astype(int) + 1\n",
        "#).clip(upper=5).astype(\"Int64\")\n",
        "\n",
        "# 1.3 Mora_estructurado\n",
        "cond_estruct_si = mora_estruct.isin([\"s√≠\", \"si\"])\n",
        "cond_estruct_no = mora_estruct.eq(\"no\")\n",
        "\n",
        "df_timeline_final.loc[cond_estruct_si, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[cond_estruct_si, \"Bucket_row\"].astype(int).clip(lower=4)\n",
        ").astype(\"Int64\")\n",
        "\n",
        "df_timeline_final.loc[cond_estruct_no & ~cond_estruct_si, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[cond_estruct_no & ~cond_estruct_si, \"Bucket_row\"].astype(int).clip(lower=3)\n",
        ").astype(\"Int64\")\n",
        "\n",
        "# 1.4 Potencial (alto aleja => peor (m√°ximo))\n",
        "p = df_timeline_final[\"Potencial\"]\n",
        "mask_p = p.notna()\n",
        "if mask_p.any():\n",
        "    p_bins = pd.qcut(p[mask_p].rank(method=\"first\"), q=6, labels=False, duplicates=\"drop\")\n",
        "    n_pbins = int(p_bins.max() + 1) if p_bins.notna().any() else 1\n",
        "    if n_pbins > 1:\n",
        "        p_scaled = np.floor(p_bins * (5 / (n_pbins - 1))).astype(int)\n",
        "    else:\n",
        "        p_scaled = pd.Series(0, index=p_bins.index, dtype=int)\n",
        "\n",
        "    bucket_por_pot = pd.Series(pd.NA, index=df_timeline_final.index, dtype=\"Int64\")\n",
        "    bucket_por_pot.loc[p_scaled.index] = p_scaled.astype(\"Int64\")\n",
        "\n",
        "    df_timeline_final.loc[mask_p, \"Bucket_row\"] = np.maximum(\n",
        "        df_timeline_final.loc[mask_p, \"Bucket_row\"].astype(int),\n",
        "        bucket_por_pot.loc[mask_p].astype(int),\n",
        "    ).astype(\"Int64\")\n",
        "\n",
        "# 1.5 Estructurable (impulso suave)\n",
        "mask_e = df_timeline_final[\"Estructurable\"].isin([0, 1])\n",
        "mask_e1 = mask_e & (df_timeline_final[\"Estructurable\"] == 1)\n",
        "mask_e0 = mask_e & (df_timeline_final[\"Estructurable\"] == 0)\n",
        "\n",
        "df_timeline_final.loc[mask_e1, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_e1, \"Bucket_row\"].astype(int) - 1\n",
        ").clip(lower=0).astype(\"Int64\")\n",
        "\n",
        "df_timeline_final.loc[mask_e0, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_e0, \"Bucket_row\"].astype(int) + 1\n",
        ").clip(upper=5).astype(\"Int64\")\n",
        "\n",
        "# 1.6 Potencial Credito (mixto)\n",
        "mask_pc_num = pc_num.notna()\n",
        "if mask_pc_num.any():\n",
        "    pc_bins = pd.qcut(pc_num[mask_pc_num].rank(method=\"first\"), q=6, labels=False, duplicates=\"drop\")\n",
        "    n_pcbins = int(pc_bins.max() + 1) if pc_bins.notna().any() else 1\n",
        "    if n_pcbins > 1:\n",
        "        pc_scaled = np.floor(pc_bins * (5 / (n_pcbins - 1))).astype(int)\n",
        "    else:\n",
        "        pc_scaled = pd.Series(0, index=pc_bins.index, dtype=int)\n",
        "\n",
        "    pc_score = pd.Series(pd.NA, index=df_timeline_final.index, dtype=\"Int64\")\n",
        "    pc_score.loc[pc_scaled.index] = pc_scaled.astype(\"Int64\")\n",
        "\n",
        "    # score 0-1 => mejora (-1)\n",
        "    mask_pc_bajo = mask_pc_num & pc_score.isin([0, 1])\n",
        "    df_timeline_final.loc[mask_pc_bajo, \"Bucket_row\"] = (\n",
        "        df_timeline_final.loc[mask_pc_bajo, \"Bucket_row\"].astype(int) - 1\n",
        "    ).clip(lower=0).astype(\"Int64\")\n",
        "\n",
        "    # score 4-5 => empeora (+1)\n",
        "    mask_pc_alto = mask_pc_num & pc_score.isin([4, 5])\n",
        "    df_timeline_final.loc[mask_pc_alto, \"Bucket_row\"] = (\n",
        "        df_timeline_final.loc[mask_pc_alto, \"Bucket_row\"].astype(int) + 1\n",
        "    ).clip(upper=5).astype(\"Int64\")\n",
        "\n",
        "# =========================================================\n",
        "# ‚úÖ CAMBIO √öNICO AQU√ç:\n",
        "# \"Tradicional\" => motiva Bucket 0 o 1 (m√°ximo 1)\n",
        "# =========================================================\n",
        "mask_pc_trad = pc_lower.eq(\"tradicional\")\n",
        "df_timeline_final.loc[mask_pc_trad, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_pc_trad, \"Bucket_row\"].astype(int).clip(upper=1)\n",
        ").astype(\"Int64\")\n",
        "\n",
        "# (Dejamos NaN/vac√≠os sin empuje especial; quedan seg√∫n otras reglas)\n",
        "\n",
        "# 1.7 Meses en el Programa + MORA\n",
        "mora = df_timeline_final[\"MORA\"]\n",
        "meses_prog = df_timeline_final[\"Meses en el Programa\"]\n",
        "\n",
        "# (A) MORA < 5 => grave => 4 o 5 din√°mico\n",
        "mask_mora_baja = mora.notna() & (mora < 5)\n",
        "if mask_mora_baja.any():\n",
        "    mora_baja = mora[mask_mora_baja]\n",
        "    mb = pd.qcut(mora_baja.rank(method=\"first\"), q=2, labels=False, duplicates=\"drop\")\n",
        "    if mb.notna().any() and int(mb.max() + 1) > 1:\n",
        "        mb_scaled = mb.map({0: 4, 1: 5}).astype(int)\n",
        "    else:\n",
        "        mb_scaled = pd.Series(4, index=mora_baja.index, dtype=int)\n",
        "\n",
        "    df_timeline_final.loc[mb_scaled.index, \"Bucket_row\"] = np.maximum(\n",
        "        df_timeline_final.loc[mb_scaled.index, \"Bucket_row\"].astype(int),\n",
        "        mb_scaled.astype(int),\n",
        "    ).astype(\"Int64\")\n",
        "\n",
        "# (B) MORA > 15 => intermedio => 3 o 4 din√°mico\n",
        "mask_mora_alta = mora.notna() & (mora > 15)\n",
        "if mask_mora_alta.any():\n",
        "    mora_alta = mora[mask_mora_alta]\n",
        "    ma = pd.qcut(mora_alta.rank(method=\"first\"), q=2, labels=False, duplicates=\"drop\")\n",
        "    if ma.notna().any() and int(ma.max() + 1) > 1:\n",
        "        ma_scaled = ma.map({0: 3, 1: 4}).astype(int)\n",
        "    else:\n",
        "        ma_scaled = pd.Series(3, index=mora_alta.index, dtype=int)\n",
        "\n",
        "    df_timeline_final.loc[ma_scaled.index, \"Bucket_row\"] = np.maximum(\n",
        "        df_timeline_final.loc[ma_scaled.index, \"Bucket_row\"].astype(int),\n",
        "        ma_scaled.astype(int),\n",
        "    ).astype(\"Int64\")\n",
        "\n",
        "# (C) MORA 6..15 y Meses>3 => impulso positivo -1\n",
        "mask_top = mora.notna() & meses_prog.notna() & mora.between(6, 15, inclusive=\"both\") & (meses_prog > 3)\n",
        "df_timeline_final.loc[mask_top, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_top, \"Bucket_row\"].astype(int) - 1\n",
        ").clip(lower=0).astype(\"Int64\")\n",
        "\n",
        "# =========================================================\n",
        "# 2) REGLAS DURAS (BUCKET 5 FIJO) ‚Äî POR REFERENCIA\n",
        "# =========================================================\n",
        "cond_credito_si = mora_credito.isin([\"s√≠\", \"si\"])\n",
        "refs_credito_si = df_timeline_final.loc[cond_credito_si, \"Referencia\"].dropna().unique()\n",
        "\n",
        "refs_no_viable_demanda = df_timeline_final.loc[\n",
        "    cat_upper.isin([\"NO_VIABLE\", \"DEMANDA\", \"ILOCALIZADO\", \"PAGO_POR_FUERA\"]),\n",
        "    \"Referencia\"\n",
        "].dropna().unique()\n",
        "\n",
        "refs_hard_5 = set(refs_credito_si).union(set(refs_no_viable_demanda))\n",
        "\n",
        "# =========================================================\n",
        "# 3) BANCOS_ESTANDAR (POR REFERENCIA) ‚Äî PEGA TU LISTA BUENO->MALO\n",
        "# =========================================================\n",
        "bancos_orden = [\n",
        "    \"Cobrando\",\n",
        "    \"Rappipay\",\n",
        "    \"Alkomprar\",\n",
        "    \"Rappicard\",\n",
        "    \"Pichincha\",\n",
        "    \"Bancolombia\",\n",
        "    \"Scotiabank Colpatria\",\n",
        "    \"Rapicredit\",\n",
        "    \"Bancoomeva\",\n",
        "    \"Codensa\",\n",
        "    \"Covinoc\",\n",
        "    \"Zinobe\",\n",
        "    \"Banco Popular\",\n",
        "    \"SisteCredito\",\n",
        "    \"Banco AV Villas\",\n",
        "    \"Colsubsidio\",\n",
        "    \"Refinancia\",\n",
        "    \"Tuya\",\n",
        "    \"Credijamar\",\n",
        "    \"Agaval\",\n",
        "    \"Alkosto\",\n",
        "    \"Banco Falabella\",\n",
        "    \"Banco de Bogot√°\",\n",
        "    \"Sistemcobro\",\n",
        "    \"Bancamia\",\n",
        "    \"Banco Finandina\",\n",
        "    \"Ita√∫\",\n",
        "    \"FGA\",\n",
        "    \"BBVA Colombia\",\n",
        "    \"Serlefin\",\n",
        "    \"Flamingo\",\n",
        "    \"Mundo Mujer\",\n",
        "    \"Banco Davivienda\",\n",
        "    \"Banco Caja Social\",\n",
        "    \"Comultrasan\",\n",
        "    \"Juriscoop\",\n",
        "    \"Credivalores\",\n",
        "    \"Banco de Occidente\",\n",
        "    \"Compensar\",\n",
        "    \"JOHN\",\n",
        "    \"Fincomercio\",\n",
        "    \"GNB Sudameris\",\n",
        "    \"Serfinanza\",\n",
        "    \"Nu\",\n",
        "    \"LuloBank\",\n",
        "    \"Aslegal\",\n",
        "    \"Baninca\",\n",
        "    \"Coltefinanciera\",\n",
        "    \"Confiar\",\n",
        "    \"Contacto soluci√≥n\",\n",
        "    \"Garantias Comunitarias\",\n",
        "    \"GRUPO JURIDICO DEUDU\",\n",
        "    \"Inversionistas Estrat√©gicos\",\n",
        "    \"Juancho te Presta\",\n",
        "    \"QNT SAS\",\n",
        "]\n",
        "\n",
        "bancos_norm = df_timeline_final[COL_BANCO].astype(\"string\").str.strip().str.lower()\n",
        "\n",
        "if len(bancos_orden) > 0:\n",
        "    banco_rank_map = {b.strip().lower(): i for i, b in enumerate(bancos_orden)}\n",
        "    rank_intermedio = len(bancos_orden) // 2\n",
        "    df_timeline_final[\"_banco_rank\"] = bancos_norm.map(banco_rank_map).fillna(rank_intermedio).astype(int)\n",
        "else:\n",
        "    df_timeline_final[\"_banco_rank\"] = 0\n",
        "\n",
        "grp_rank = df_timeline_final.groupby(\"Referencia\")[\"_banco_rank\"]\n",
        "best_rank = grp_rank.min()\n",
        "worst_rank = grp_rank.max()\n",
        "\n",
        "score_banco = (ALPHA_BEST_BANK * best_rank) + ((1 - ALPHA_BEST_BANK) * worst_rank)\n",
        "\n",
        "tmp = score_banco.rank(method=\"first\")\n",
        "bb = pd.qcut(tmp, q=6, labels=False, duplicates=\"drop\")\n",
        "n_bb = int(bb.max() + 1) if bb.notna().any() else 1\n",
        "if n_bb > 1:\n",
        "    bucket_banco = np.floor(bb * (5 / (n_bb - 1))).astype(int)\n",
        "else:\n",
        "    bucket_banco = pd.Series(0, index=score_banco.index, dtype=int)\n",
        "bucket_banco = bucket_banco.clip(0, 5).astype(\"Int64\")\n",
        "bucket_banco.name = \"Bucket_Banco\"\n",
        "\n",
        "# =========================================================\n",
        "# 4) CRITERIO: max(Ahorro total / D_BRAVO) POR REFERENCIA\n",
        "#    (alto ratio => empuja hacia bucket bajo, bajo ratio => hacia bucket alto)\n",
        "# =========================================================\n",
        "df_timeline_final[\"ratio_ahorro_db\"] = np.where(\n",
        "    (df_timeline_final[\"D_BRAVO\"].notna()) & (df_timeline_final[\"D_BRAVO\"] > 0) & df_timeline_final[\"Ahorro total\"].notna(),\n",
        "    df_timeline_final[\"Ahorro total\"] / df_timeline_final[\"D_BRAVO\"],\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "ratio_max_ref = df_timeline_final.groupby(\"Referencia\")[\"ratio_ahorro_db\"].max()\n",
        "\n",
        "mask_ratio = ratio_max_ref.notna()\n",
        "if mask_ratio.any():\n",
        "    rr = pd.qcut(ratio_max_ref[mask_ratio].rank(method=\"first\"), q=6, labels=False, duplicates=\"drop\")\n",
        "    n_rr = int(rr.max() + 1) if rr.notna().any() else 1\n",
        "    if n_rr > 1:\n",
        "        rr_scaled = np.floor(rr * (5 / (n_rr - 1))).astype(int)\n",
        "    else:\n",
        "        rr_scaled = pd.Series(0, index=rr.index, dtype=int)\n",
        "\n",
        "    bucket_ratio = pd.Series(pd.NA, index=ratio_max_ref.index, dtype=\"Int64\")\n",
        "    bucket_ratio.loc[rr_scaled.index] = (5 - rr_scaled).astype(\"Int64\")  # invertido: alto ratio => bucket bajo\n",
        "else:\n",
        "    bucket_ratio = pd.Series(pd.NA, index=ratio_max_ref.index, dtype=\"Int64\")\n",
        "\n",
        "bucket_ratio.name = \"Bucket_Ratio\"\n",
        "\n",
        "# =========================================================\n",
        "# 5) TABLA POR REFERENCIA: SCORE CONTINUO (para ordenar dentro de negociador)\n",
        "# =========================================================\n",
        "bucket_ref_base = df_timeline_final.groupby(\"Referencia\")[\"Bucket_row\"].max().astype(\"Int64\")\n",
        "bucket_ref_base.name = \"bucket_ref_base\"\n",
        "\n",
        "ref_table = pd.concat([bucket_ref_base, bucket_banco, bucket_ratio], axis=1)\n",
        "ref_table.columns = [\"bucket_ref_base\", \"Bucket_Banco\", \"Bucket_Ratio\"]\n",
        "\n",
        "# Fill intermedios si falta info (NO castiga ni premia)\n",
        "ref_table[\"bucket_ref_base\"] = ref_table[\"bucket_ref_base\"].fillna(0)\n",
        "ref_table[\"Bucket_Banco\"] = ref_table[\"Bucket_Banco\"].fillna(3)\n",
        "ref_table[\"Bucket_Ratio\"] = ref_table[\"Bucket_Ratio\"].fillna(3)\n",
        "\n",
        "# score continuo: menor = m√°s importante\n",
        "# (ratio ya viene como bucket 0..5 invertido, as√≠ que entra directo)\n",
        "ref_table[\"score_ref\"] = (\n",
        "    (W_BASE * ref_table[\"bucket_ref_base\"].astype(float)) +\n",
        "    (W_BANK * ref_table[\"Bucket_Banco\"].astype(float)) +\n",
        "    (0.20 * ref_table[\"Bucket_Ratio\"].astype(float))  # impulso ahorro/DBRAVO\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 5.1) REGRA DURA: D_BRAVO MUY ALTO => BUCKET 0 o 1\n",
        "# =========================================================\n",
        "\n",
        "# promedio global de D_BRAVO\n",
        "mean_dbravo = df_timeline_final[\"D_BRAVO\"].mean()\n",
        "\n",
        "# referencias con al menos una deuda 90% > promedio\n",
        "refs_dbravo_alto = (\n",
        "    df_timeline_final.loc[\n",
        "        df_timeline_final[\"D_BRAVO\"] >= 1.9 * mean_dbravo,\n",
        "        \"Referencia\"\n",
        "    ]\n",
        "    .dropna()\n",
        "    .unique()\n",
        ")\n",
        "\n",
        "# referencias con Banco Davivienda\n",
        "refs_davivienda = (\n",
        "    df_timeline_final.loc[\n",
        "        df_timeline_final[COL_BANCO]\n",
        "        .astype(\"string\")\n",
        "        .str.strip()\n",
        "        .str.lower()\n",
        "        .eq(\"banco davivienda\"),\n",
        "        \"Referencia\"\n",
        "    ]\n",
        "    .dropna()\n",
        "    .unique()\n",
        ")\n",
        "\n",
        "# elegibles: alto D_BRAVO, no hard 5, no Davivienda\n",
        "refs_dbravo_prioridad = (\n",
        "    set(refs_dbravo_alto)\n",
        "    - set(refs_hard_5)\n",
        "    - set(refs_davivienda)\n",
        ")\n",
        "\n",
        "# aplicar regla:\n",
        "# si bucket_base <= 2 => Bucket 0\n",
        "# si bucket_base > 2  => Bucket 1\n",
        "mask_prior = ref_table.index.isin(refs_dbravo_prioridad)\n",
        "\n",
        "ref_table.loc[mask_prior & (ref_table[\"bucket_ref_base\"] <= 2), \"bucket_ref_base\"] = 0\n",
        "ref_table.loc[mask_prior & (ref_table[\"bucket_ref_base\"] > 2), \"bucket_ref_base\"] = 1\n",
        "\n",
        "# =========================================================\n",
        "# 6) MAPEAR NEGOCIADOR POR REFERENCIA\n",
        "#    (si una referencia aparece con varios negociadores, toma el m√°s frecuente)\n",
        "# =========================================================\n",
        "ref_neg = (\n",
        "    df_timeline_final.groupby([\"Referencia\", COL_NEGOCIADOR]).size()\n",
        "    .reset_index(name=\"n\")\n",
        "    .sort_values([\"Referencia\", \"n\"], ascending=[True, False])\n",
        "    .drop_duplicates(\"Referencia\")\n",
        "    .set_index(\"Referencia\")[COL_NEGOCIADOR]\n",
        ")\n",
        "\n",
        "ref_table = ref_table.join(ref_neg.rename(\"negociador_ref\"), how=\"left\")\n",
        "\n",
        "# =========================================================\n",
        "# 7) ASIGNACI√ìN EQUITATIVA POR NEGOCIADOR (CUOTAS)\n",
        "#    Respeta hard=5. El resto se distribuye por score (mejor score => bucket m√°s bajo).\n",
        "# =========================================================\n",
        "def _target_counts(n_total: int) -> dict:\n",
        "    # floor + repartir sobrantes por los buckets m√°s importantes primero\n",
        "    raw = {b: int(np.floor(TARGET_PCTS[b] * n_total)) for b in BUCKETS}\n",
        "    missing = n_total - sum(raw.values())\n",
        "    # repartir sobrantes: 0,1,2,3,4,5\n",
        "    for b in BUCKETS:\n",
        "        if missing <= 0:\n",
        "            break\n",
        "        raw[b] += 1\n",
        "        missing -= 1\n",
        "    return raw\n",
        "\n",
        "bucket_final = pd.Series(index=ref_table.index, dtype=\"Int64\")\n",
        "\n",
        "for neg, sub in ref_table.groupby(\"negociador_ref\", dropna=False):\n",
        "    refs = sub.index.tolist()\n",
        "    n_total = len(refs)\n",
        "    if n_total == 0:\n",
        "        continue\n",
        "\n",
        "    targets = _target_counts(n_total)\n",
        "\n",
        "    # hard 5 dentro del negociador\n",
        "    hard_mask = [r in refs_hard_5 for r in refs]\n",
        "    hard_refs = sub.index[hard_mask]\n",
        "    n_hard = len(hard_refs)\n",
        "\n",
        "    # asignar hard primero\n",
        "    if n_hard > 0:\n",
        "        bucket_final.loc[hard_refs] = 5\n",
        "\n",
        "    # refs elegibles (no hard)\n",
        "    free = sub.loc[~pd.Index(refs).isin(hard_refs)].copy()\n",
        "    n_free = len(free)\n",
        "\n",
        "    if n_free == 0:\n",
        "        continue\n",
        "\n",
        "    # ajustar cuota bucket 5 para lo que falte (si hard ya ocup√≥ parte)\n",
        "    # si hard excede target5, dejamos bucket5 = hard y recortamos otros buckets desde el 4 hacia abajo\n",
        "    if n_hard > targets[5]:\n",
        "        extra = n_hard - targets[5]\n",
        "        targets[5] = n_hard\n",
        "        # recortar extra de 4,3,2,1,0 en ese orden\n",
        "        for b in [4, 3, 2, 1, 0]:\n",
        "            if extra <= 0:\n",
        "                break\n",
        "            take = min(targets[b], extra)\n",
        "            targets[b] -= take\n",
        "            extra -= take\n",
        "    else:\n",
        "        targets[5] = targets[5] - n_hard  # lo que queda por llenar con refs libres\n",
        "\n",
        "    # si por redondeos qued√≥ suma distinta a n_free, ajustar en bucket 4 (neutro)\n",
        "    total_needed = sum(targets.values())\n",
        "    if total_needed != n_free:\n",
        "        diff = n_free - total_needed\n",
        "        targets[4] = max(0, targets[4] + diff)\n",
        "\n",
        "    # ordenar por score (menor=mejor)\n",
        "    free = free.sort_values(\"score_ref\", ascending=True)\n",
        "\n",
        "    # asignar por bloques\n",
        "    start = 0\n",
        "    for b in BUCKETS:\n",
        "        cnt = targets[b]\n",
        "        if cnt <= 0:\n",
        "            continue\n",
        "        end = start + cnt\n",
        "        bucket_final.loc[free.index[start:end]] = b\n",
        "        start = end\n",
        "\n",
        "# si alguna referencia qued√≥ sin asignar (por negociador nulo o edge cases), cae a bucket por score global\n",
        "mask_na = bucket_final.isna()\n",
        "if mask_na.any():\n",
        "    tmp = ref_table.loc[mask_na].sort_values(\"score_ref\", ascending=True)\n",
        "    # asignaci√≥n simple por quantiles globales\n",
        "    ranks = tmp[\"score_ref\"].rank(method=\"first\")\n",
        "    q = pd.qcut(ranks, q=6, labels=False, duplicates=\"drop\")\n",
        "    n_q = int(q.max() + 1) if q.notna().any() else 1\n",
        "    if n_q > 1:\n",
        "        q_scaled = np.floor(q * (5 / (n_q - 1))).astype(int)\n",
        "    else:\n",
        "        q_scaled = pd.Series(0, index=tmp.index, dtype=int)\n",
        "    bucket_final.loc[tmp.index] = q_scaled.clip(0, 5).astype(\"Int64\")\n",
        "    # hard sigue siendo 5\n",
        "    bucket_final.loc[list(refs_hard_5)] = 5\n",
        "\n",
        "ref_table[\"Bucket_ref\"] = bucket_final.astype(\"Int64\")\n",
        "\n",
        "# =========================================================\n",
        "# 8) PEGAR A TODAS LAS FILAS (UNA REFERENCIA = UN BUCKET)\n",
        "# =========================================================\n",
        "df_timeline_final = df_timeline_final.merge(\n",
        "    ref_table[[\"Bucket_ref\"]],\n",
        "    left_on=\"Referencia\",\n",
        "    right_index=True,\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "df_timeline_final[\"Bucket\"] = df_timeline_final[\"Bucket_ref\"].astype(\"Int64\")\n",
        "\n",
        "# Limpieza auxiliares\n",
        "df_timeline_final.drop(\n",
        "    columns=[\"Bucket_ref\", \"Bucket_row\", \"_banco_rank\", \"ratio_ahorro_db\"],\n",
        "    inplace=True,\n",
        "    errors=\"ignore\",\n",
        ")\n",
        "\n",
        "df_timeline_final[\"Bucket\"] = df_timeline_final[\"Bucket\"].astype(int).clip(0, 5).astype(\"Int64\")"
      ],
      "metadata": {
        "id": "WB-HjdnE4D03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ESTE CODIGO PARA PLUS EL PROXIMO MES: import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =========================================================\n",
        "# CONFIG\n",
        "# =========================================================\n",
        "COL_ULT_CONTACTO = \"ultimo contacto\"   # <- as√≠, con espacio\n",
        "COL_NEGOCIADOR = \"Negociador\"          # <- AJUSTA si tu columna se llama distinto\n",
        "COL_BANCO = \"BANCOS_ESTANDAR\"\n",
        "\n",
        "# Distribuci√≥n objetivo por negociador (default)\n",
        "TARGET_PCTS_DEFAULT = {0: 0.10, 1: 0.10, 2: 0.15, 3: 0.15, 4: 0.25, 5: 0.25}\n",
        "\n",
        "# Distribuci√≥n para Premium\n",
        "TARGET_PCTS_PREMIUM = {0: 0.20, 1: 0.20, 2: 0.20, 3: 0.10, 4: 0.15, 5: 0.15}\n",
        "\n",
        "BUCKETS = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "# Pesos para score final (puedes ajustarlos)\n",
        "W_BASE = 0.75   # peso de lo ya construido (Bucket_row consolidado)\n",
        "W_BANK = 0.25   # peso del banco\n",
        "# Ahorro/D_BRAVO entra como impulso suave por buckets (ver m√°s abajo)\n",
        "\n",
        "# M√°s peso al mejor banco que al peor (para el criterio banco)\n",
        "ALPHA_BEST_BANK = 0.70\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 0) MAPEO Negociador -> Tipo (desde df_asignacion)\n",
        "#     (Premium vs resto)\n",
        "# =========================================================\n",
        "# df_asignacion debe tener columnas: \"Negociador\" y \"Tipo\"\n",
        "neg_tipo_map = (\n",
        "    df_asignacion[[\"Negociador\", \"Tipo\"]]\n",
        "    .dropna()\n",
        "    .assign(\n",
        "        Negociador=lambda x: x[\"Negociador\"].astype(str).str.strip(),\n",
        "        Tipo=lambda x: x[\"Tipo\"].astype(str).str.strip(),\n",
        "    )\n",
        "    .drop_duplicates(\"Negociador\")\n",
        "    .set_index(\"Negociador\")[\"Tipo\"]\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 0) COPIA + LIMPIEZA / NORMALIZACI√ìN\n",
        "# =========================================================\n",
        "df_timeline_final = df_timeline_final.copy()\n",
        "\n",
        "# Num√©ricas\n",
        "df_timeline_final[COL_ULT_CONTACTO] = pd.to_numeric(df_timeline_final[COL_ULT_CONTACTO], errors=\"coerce\").fillna(6)\n",
        "df_timeline_final[\"Potencial\"] = pd.to_numeric(df_timeline_final[\"Potencial\"], errors=\"coerce\")\n",
        "df_timeline_final[\"Estructurable\"] = pd.to_numeric(df_timeline_final[\"Estructurable\"], errors=\"coerce\")\n",
        "df_timeline_final[\"MORA\"] = pd.to_numeric(df_timeline_final[\"MORA\"], errors=\"coerce\")\n",
        "df_timeline_final[\"Meses en el Programa\"] = pd.to_numeric(df_timeline_final[\"Meses en el Programa\"], errors=\"coerce\")\n",
        "df_timeline_final[\"D_BRAVO\"] = pd.to_numeric(df_timeline_final[\"D_BRAVO\"], errors=\"coerce\")\n",
        "df_timeline_final[\"Ahorro total\"] = pd.to_numeric(df_timeline_final[\"Ahorro total\"], errors=\"coerce\")\n",
        "\n",
        "# Textos\n",
        "mora_credito = df_timeline_final[\"MORA_CREDITO\"].astype(\"string\").str.strip().str.lower()\n",
        "mora_estruct = df_timeline_final[\"Mora_estructurado\"].astype(\"string\").str.strip().str.lower()\n",
        "cat_pred = df_timeline_final[\"CATEGORIA_PRED\"].astype(\"string\").str.strip()\n",
        "cat_upper = cat_pred.str.upper()\n",
        "\n",
        "# Potencial Credito mixto\n",
        "pc_raw = df_timeline_final[\"Potencial Credito\"].astype(\"string\").str.strip()\n",
        "pc_lower = pc_raw.str.lower()\n",
        "pc_num = pd.to_numeric(pc_raw, errors=\"coerce\")\n",
        "\n",
        "# =========================================================\n",
        "# 1) BUCKET \"POR FILA\" (PASO INTERMEDIO) => Bucket_row (0..5)\n",
        "# =========================================================\n",
        "# 1.1 Base por recencia (din√°mico 0..5)\n",
        "s = df_timeline_final[COL_ULT_CONTACTO]\n",
        "base = pd.qcut(s.rank(method=\"first\"), q=6, labels=False, duplicates=\"drop\")\n",
        "n_bins = int(base.max() + 1) if base.notna().any() else 1\n",
        "if n_bins > 1:\n",
        "    base_scaled = np.floor(base * (5 / (n_bins - 1))).astype(int)\n",
        "else:\n",
        "    base_scaled = pd.Series(0, index=df_timeline_final.index, dtype=int)\n",
        "df_timeline_final[\"Bucket_row\"] = base_scaled.astype(\"Int64\")\n",
        "\n",
        "# 1.2 Penalizaci√≥n por cliente con al menos 1 \"Liquidado\" (suave +1)\n",
        "clientes_con_liquidado = df_timeline_final.loc[cat_pred.eq(\"Liquidado\"), \"Referencia\"].dropna().unique()\n",
        "mask_cliente_liquidado = df_timeline_final[\"Referencia\"].isin(clientes_con_liquidado)\n",
        "df_timeline_final.loc[mask_cliente_liquidado, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_cliente_liquidado, \"Bucket_row\"].astype(int) + 1\n",
        ").clip(upper=5).astype(\"Int64\")\n",
        "\n",
        "# 1.3 Mora_estructurado\n",
        "cond_estruct_si = mora_estruct.isin([\"s√≠\", \"si\"])\n",
        "cond_estruct_no = mora_estruct.eq(\"no\")\n",
        "\n",
        "df_timeline_final.loc[cond_estruct_si, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[cond_estruct_si, \"Bucket_row\"].astype(int).clip(lower=4)\n",
        ").astype(\"Int64\")\n",
        "\n",
        "df_timeline_final.loc[cond_estruct_no & ~cond_estruct_si, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[cond_estruct_no & ~cond_estruct_si, \"Bucket_row\"].astype(int).clip(lower=3)\n",
        ").astype(\"Int64\")\n",
        "\n",
        "# 1.4 Potencial (alto aleja => peor (m√°ximo))\n",
        "p = df_timeline_final[\"Potencial\"]\n",
        "mask_p = p.notna()\n",
        "if mask_p.any():\n",
        "    p_bins = pd.qcut(p[mask_p].rank(method=\"first\"), q=6, labels=False, duplicates=\"drop\")\n",
        "    n_pbins = int(p_bins.max() + 1) if p_bins.notna().any() else 1\n",
        "    if n_pbins > 1:\n",
        "        p_scaled = np.floor(p_bins * (5 / (n_pbins - 1))).astype(int)\n",
        "    else:\n",
        "        p_scaled = pd.Series(0, index=p_bins.index, dtype=int)\n",
        "\n",
        "    bucket_por_pot = pd.Series(pd.NA, index=df_timeline_final.index, dtype=\"Int64\")\n",
        "    bucket_por_pot.loc[p_scaled.index] = p_scaled.astype(\"Int64\")\n",
        "\n",
        "    df_timeline_final.loc[mask_p, \"Bucket_row\"] = np.maximum(\n",
        "        df_timeline_final.loc[mask_p, \"Bucket_row\"].astype(int),\n",
        "        bucket_por_pot.loc[mask_p].astype(int),\n",
        "    ).astype(\"Int64\")\n",
        "\n",
        "# 1.5 Estructurable (impulso suave)\n",
        "mask_e = df_timeline_final[\"Estructurable\"].isin([0, 1])\n",
        "mask_e1 = mask_e & (df_timeline_final[\"Estructurable\"] == 1)\n",
        "mask_e0 = mask_e & (df_timeline_final[\"Estructurable\"] == 0)\n",
        "\n",
        "df_timeline_final.loc[mask_e1, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_e1, \"Bucket_row\"].astype(int) - 1\n",
        ").clip(lower=0).astype(\"Int64\")\n",
        "\n",
        "df_timeline_final.loc[mask_e0, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_e0, \"Bucket_row\"].astype(int) + 1\n",
        ").clip(upper=5).astype(\"Int64\")\n",
        "\n",
        "# 1.6 Potencial Credito (mixto)\n",
        "mask_pc_num = pc_num.notna()\n",
        "if mask_pc_num.any():\n",
        "    pc_bins = pd.qcut(pc_num[mask_pc_num].rank(method=\"first\"), q=6, labels=False, duplicates=\"drop\")\n",
        "    n_pcbins = int(pc_bins.max() + 1) if pc_bins.notna().any() else 1\n",
        "    if n_pcbins > 1:\n",
        "        pc_scaled = np.floor(pc_bins * (5 / (n_pcbins - 1))).astype(int)\n",
        "    else:\n",
        "        pc_scaled = pd.Series(0, index=pc_bins.index, dtype=int)\n",
        "\n",
        "    pc_score = pd.Series(pd.NA, index=df_timeline_final.index, dtype=\"Int64\")\n",
        "    pc_score.loc[pc_scaled.index] = pc_scaled.astype(\"Int64\")\n",
        "\n",
        "    # score 0-1 => mejora (-1)\n",
        "    mask_pc_bajo = mask_pc_num & pc_score.isin([0, 1])\n",
        "    df_timeline_final.loc[mask_pc_bajo, \"Bucket_row\"] = (\n",
        "        df_timeline_final.loc[mask_pc_bajo, \"Bucket_row\"].astype(int) - 1\n",
        "    ).clip(lower=0).astype(\"Int64\")\n",
        "\n",
        "    # score 4-5 => empeora (+1)\n",
        "    mask_pc_alto = mask_pc_num & pc_score.isin([4, 5])\n",
        "    df_timeline_final.loc[mask_pc_alto, \"Bucket_row\"] = (\n",
        "        df_timeline_final.loc[mask_pc_alto, \"Bucket_row\"].astype(int) + 1\n",
        "    ).clip(upper=5).astype(\"Int64\")\n",
        "\n",
        "# Tradicional o NaN => intermedio (m√≠nimo 3)\n",
        "mask_pc_trad = pc_lower.eq(\"tradicional\")\n",
        "mask_pc_nan = pc_raw.isna() | pc_lower.isin([\"<na>\", \"nan\", \"none\", \"\"])\n",
        "mask_pc_intermedio = (mask_pc_trad | mask_pc_nan) & (~mask_pc_num)\n",
        "df_timeline_final.loc[mask_pc_intermedio, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_pc_intermedio, \"Bucket_row\"].astype(int).clip(lower=3)\n",
        ").astype(\"Int64\")\n",
        "\n",
        "# 1.7 Meses en el Programa + MORA\n",
        "mora = df_timeline_final[\"MORA\"]\n",
        "meses_prog = df_timeline_final[\"Meses en el Programa\"]\n",
        "\n",
        "# (A) MORA < 5 => grave => 4 o 5 din√°mico\n",
        "mask_mora_baja = mora.notna() & (mora < 5)\n",
        "if mask_mora_baja.any():\n",
        "    mora_baja = mora[mask_mora_baja]\n",
        "    mb = pd.qcut(mora_baja.rank(method=\"first\"), q=2, labels=False, duplicates=\"drop\")\n",
        "    if mb.notna().any() and int(mb.max() + 1) > 1:\n",
        "        mb_scaled = mb.map({0: 4, 1: 5}).astype(int)\n",
        "    else:\n",
        "        mb_scaled = pd.Series(4, index=mora_baja.index, dtype=int)\n",
        "\n",
        "    df_timeline_final.loc[mb_scaled.index, \"Bucket_row\"] = np.maximum(\n",
        "        df_timeline_final.loc[mb_scaled.index, \"Bucket_row\"].astype(int),\n",
        "        mb_scaled.astype(int),\n",
        "    ).astype(\"Int64\")\n",
        "\n",
        "# (B) MORA > 15 => intermedio => 3 o 4 din√°mico\n",
        "mask_mora_alta = mora.notna() & (mora > 15)\n",
        "if mask_mora_alta.any():\n",
        "    mora_alta = mora[mask_mora_alta]\n",
        "    ma = pd.qcut(mora_alta.rank(method=\"first\"), q=2, labels=False, duplicates=\"drop\")\n",
        "    if ma.notna().any() and int(ma.max() + 1) > 1:\n",
        "        ma_scaled = ma.map({0: 3, 1: 4}).astype(int)\n",
        "    else:\n",
        "        ma_scaled = pd.Series(3, index=mora_alta.index, dtype=int)\n",
        "\n",
        "    df_timeline_final.loc[ma_scaled.index, \"Bucket_row\"] = np.maximum(\n",
        "        df_timeline_final.loc[ma_scaled.index, \"Bucket_row\"].astype(int),\n",
        "        ma_scaled.astype(int),\n",
        "    ).astype(\"Int64\")\n",
        "\n",
        "# (C) MORA 6..15 y Meses>3 => impulso positivo -1\n",
        "mask_top = mora.notna() & meses_prog.notna() & mora.between(6, 15, inclusive=\"both\") & (meses_prog > 3)\n",
        "df_timeline_final.loc[mask_top, \"Bucket_row\"] = (\n",
        "    df_timeline_final.loc[mask_top, \"Bucket_row\"].astype(int) - 1\n",
        ").clip(lower=0).astype(\"Int64\")\n",
        "\n",
        "# =========================================================\n",
        "# 2) REGLAS DURAS (BUCKET 5 FIJO) ‚Äî POR REFERENCIA\n",
        "# =========================================================\n",
        "cond_credito_si = mora_credito.isin([\"s√≠\", \"si\"])\n",
        "refs_credito_si = df_timeline_final.loc[cond_credito_si, \"Referencia\"].dropna().unique()\n",
        "\n",
        "refs_no_viable_demanda = df_timeline_final.loc[\n",
        "    cat_upper.isin([\"NO_VIABLE\", \"DEMANDA\", \"ILOCALIZADO\", \"PAGO_POR_FUERA\"]),\n",
        "    \"Referencia\"\n",
        "].dropna().unique()\n",
        "\n",
        "refs_hard_5 = set(refs_credito_si).union(set(refs_no_viable_demanda))\n",
        "\n",
        "# =========================================================\n",
        "# 3) BANCOS_ESTANDAR (POR REFERENCIA) ‚Äî PEGA TU LISTA BUENO->MALO\n",
        "# =========================================================\n",
        "bancos_orden = [\n",
        "    \"Cobrando\",\"Rappipay\",\"Alkomprar\",\"Rappicard\",\"Pichincha\",\"Bancolombia\",\n",
        "    \"Scotiabank Colpatria\",\"Rapicredit\",\"Bancoomeva\",\"Codensa\",\"Covinoc\",\"Zinobe\",\n",
        "    \"Banco Popular\",\"SisteCredito\",\"Banco AV Villas\",\"Colsubsidio\",\"Refinancia\",\"Tuya\",\n",
        "    \"Credijamar\",\"Agaval\",\"Alkosto\",\"Banco Falabella\",\"Banco de Bogot√°\",\"Sistemcobro\",\n",
        "    \"Bancamia\",\"Banco Finandina\",\"Ita√∫\",\"FGA\",\"BBVA Colombia\",\"Serlefin\",\"Flamingo\",\n",
        "    \"Mundo Mujer\",\"Banco Davivienda\",\"Banco Caja Social\",\"Comultrasan\",\"Juriscoop\",\n",
        "    \"Credivalores\",\"Banco de Occidente\",\"Compensar\",\"JOHN\",\"Fincomercio\",\"GNB Sudameris\",\n",
        "    \"Serfinanza\",\"Nu\",\"LuloBank\",\"Aslegal\",\"Baninca\",\"Coltefinanciera\",\"Confiar\",\n",
        "    \"Contacto soluci√≥n\",\"Garantias Comunitarias\",\"GRUPO JURIDICO DEUDU\",\n",
        "    \"Inversionistas Estrat√©gicos\",\"Juancho te Presta\",\"QNT SAS\",\n",
        "]\n",
        "\n",
        "bancos_norm = df_timeline_final[COL_BANCO].astype(\"string\").str.strip().str.lower()\n",
        "\n",
        "if len(bancos_orden) > 0:\n",
        "    banco_rank_map = {b.strip().lower(): i for i, b in enumerate(bancos_orden)}\n",
        "    rank_intermedio = len(bancos_orden) // 2\n",
        "    df_timeline_final[\"_banco_rank\"] = bancos_norm.map(banco_rank_map).fillna(rank_intermedio).astype(int)\n",
        "else:\n",
        "    df_timeline_final[\"_banco_rank\"] = 0\n",
        "\n",
        "grp_rank = df_timeline_final.groupby(\"Referencia\")[\"_banco_rank\"]\n",
        "best_rank = grp_rank.min()\n",
        "worst_rank = grp_rank.max()\n",
        "\n",
        "score_banco = (ALPHA_BEST_BANK * best_rank) + ((1 - ALPHA_BEST_BANK) * worst_rank)\n",
        "\n",
        "tmp = score_banco.rank(method=\"first\")\n",
        "bb = pd.qcut(tmp, q=6, labels=False, duplicates=\"drop\")\n",
        "n_bb = int(bb.max() + 1) if bb.notna().any() else 1\n",
        "if n_bb > 1:\n",
        "    bucket_banco = np.floor(bb * (5 / (n_bb - 1))).astype(int)\n",
        "else:\n",
        "    bucket_banco = pd.Series(0, index=score_banco.index, dtype=int)\n",
        "bucket_banco = bucket_banco.clip(0, 5).astype(\"Int64\")\n",
        "bucket_banco.name = \"Bucket_Banco\"\n",
        "\n",
        "# =========================================================\n",
        "# 4) CRITERIO: max(Ahorro total / D_BRAVO) POR REFERENCIA\n",
        "# =========================================================\n",
        "df_timeline_final[\"ratio_ahorro_db\"] = np.where(\n",
        "    (df_timeline_final[\"D_BRAVO\"].notna()) & (df_timeline_final[\"D_BRAVO\"] > 0) & df_timeline_final[\"Ahorro total\"].notna(),\n",
        "    df_timeline_final[\"Ahorro total\"] / df_timeline_final[\"D_BRAVO\"],\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "ratio_max_ref = df_timeline_final.groupby(\"Referencia\")[\"ratio_ahorro_db\"].max()\n",
        "\n",
        "mask_ratio = ratio_max_ref.notna()\n",
        "if mask_ratio.any():\n",
        "    rr = pd.qcut(ratio_max_ref[mask_ratio].rank(method=\"first\"), q=6, labels=False, duplicates=\"drop\")\n",
        "    n_rr = int(rr.max() + 1) if rr.notna().any() else 1\n",
        "    if n_rr > 1:\n",
        "        rr_scaled = np.floor(rr * (5 / (n_rr - 1))).astype(int)\n",
        "    else:\n",
        "        rr_scaled = pd.Series(0, index=rr.index, dtype=int)\n",
        "\n",
        "    bucket_ratio = pd.Series(pd.NA, index=ratio_max_ref.index, dtype=\"Int64\")\n",
        "    bucket_ratio.loc[rr_scaled.index] = (5 - rr_scaled).astype(\"Int64\")\n",
        "else:\n",
        "    bucket_ratio = pd.Series(pd.NA, index=ratio_max_ref.index, dtype=\"Int64\")\n",
        "\n",
        "bucket_ratio.name = \"Bucket_Ratio\"\n",
        "\n",
        "# =========================================================\n",
        "# 5) TABLA POR REFERENCIA: SCORE CONTINUO\n",
        "# =========================================================\n",
        "bucket_ref_base = df_timeline_final.groupby(\"Referencia\")[\"Bucket_row\"].max().astype(\"Int64\")\n",
        "bucket_ref_base.name = \"bucket_ref_base\"\n",
        "\n",
        "ref_table = pd.concat([bucket_ref_base, bucket_banco, bucket_ratio], axis=1)\n",
        "ref_table.columns = [\"bucket_ref_base\", \"Bucket_Banco\", \"Bucket_Ratio\"]\n",
        "\n",
        "ref_table[\"bucket_ref_base\"] = ref_table[\"bucket_ref_base\"].fillna(0)\n",
        "ref_table[\"Bucket_Banco\"] = ref_table[\"Bucket_Banco\"].fillna(3)\n",
        "ref_table[\"Bucket_Ratio\"] = ref_table[\"Bucket_Ratio\"].fillna(3)\n",
        "\n",
        "ref_table[\"score_ref\"] = (\n",
        "    (W_BASE * ref_table[\"bucket_ref_base\"].astype(float)) +\n",
        "    (W_BANK * ref_table[\"Bucket_Banco\"].astype(float)) +\n",
        "    (0.20 * ref_table[\"Bucket_Ratio\"].astype(float))\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 5.1) REGRA DURA: D_BRAVO MUY ALTO => BUCKET 0 o 1\n",
        "# =========================================================\n",
        "mean_dbravo = df_timeline_final[\"D_BRAVO\"].mean()\n",
        "\n",
        "refs_dbravo_alto = (\n",
        "    df_timeline_final.loc[df_timeline_final[\"D_BRAVO\"] >= 1.9 * mean_dbravo, \"Referencia\"]\n",
        "    .dropna().unique()\n",
        ")\n",
        "\n",
        "refs_davivienda = (\n",
        "    df_timeline_final.loc[\n",
        "        df_timeline_final[COL_BANCO].astype(\"string\").str.strip().str.lower().eq(\"banco davivienda\"),\n",
        "        \"Referencia\"\n",
        "    ]\n",
        "    .dropna().unique()\n",
        ")\n",
        "\n",
        "refs_dbravo_prioridad = set(refs_dbravo_alto) - set(refs_hard_5) - set(refs_davivienda)\n",
        "\n",
        "mask_prior = ref_table.index.isin(refs_dbravo_prioridad)\n",
        "ref_table.loc[mask_prior & (ref_table[\"bucket_ref_base\"] <= 2), \"bucket_ref_base\"] = 0\n",
        "ref_table.loc[mask_prior & (ref_table[\"bucket_ref_base\"] > 2), \"bucket_ref_base\"] = 1\n",
        "\n",
        "# OJO: como bucket_ref_base cambi√≥, re-calculamos score_ref\n",
        "ref_table[\"score_ref\"] = (\n",
        "    (W_BASE * ref_table[\"bucket_ref_base\"].astype(float)) +\n",
        "    (W_BANK * ref_table[\"Bucket_Banco\"].astype(float)) +\n",
        "    (0.20 * ref_table[\"Bucket_Ratio\"].astype(float))\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 6) MAPEAR NEGOCIADOR POR REFERENCIA\n",
        "# =========================================================\n",
        "ref_neg = (\n",
        "    df_timeline_final.groupby([\"Referencia\", COL_NEGOCIADOR]).size()\n",
        "    .reset_index(name=\"n\")\n",
        "    .sort_values([\"Referencia\", \"n\"], ascending=[True, False])\n",
        "    .drop_duplicates(\"Referencia\")\n",
        "    .set_index(\"Referencia\")[COL_NEGOCIADOR]\n",
        ")\n",
        "\n",
        "ref_table = ref_table.join(ref_neg.rename(\"negociador_ref\"), how=\"left\")\n",
        "\n",
        "# =========================================================\n",
        "# 7) ASIGNACI√ìN EQUITATIVA POR NEGOCIADOR (CUOTAS) + PREMIUM\n",
        "# =========================================================\n",
        "def _target_counts(n_total: int, pcts: dict) -> dict:\n",
        "    raw = {b: int(np.floor(pcts[b] * n_total)) for b in BUCKETS}\n",
        "    missing = n_total - sum(raw.values())\n",
        "    for b in BUCKETS:\n",
        "        if missing <= 0:\n",
        "            break\n",
        "        raw[b] += 1\n",
        "        missing -= 1\n",
        "    return raw\n",
        "\n",
        "bucket_final = pd.Series(index=ref_table.index, dtype=\"Int64\")\n",
        "\n",
        "for neg, sub in ref_table.groupby(\"negociador_ref\", dropna=False):\n",
        "    refs = sub.index.tolist()\n",
        "    n_total = len(refs)\n",
        "    if n_total == 0:\n",
        "        continue\n",
        "\n",
        "    # Elegir distribuci√≥n seg√∫n tipo\n",
        "    tipo_neg = neg_tipo_map.get(str(neg).strip(), \"\")\n",
        "    pcts = TARGET_PCTS_PREMIUM if tipo_neg == \"Premium\" else TARGET_PCTS_DEFAULT\n",
        "\n",
        "    targets = _target_counts(n_total, pcts)\n",
        "\n",
        "    # hard 5 dentro del negociador\n",
        "    hard_mask = [r in refs_hard_5 for r in refs]\n",
        "    hard_refs = sub.index[hard_mask]\n",
        "    n_hard = len(hard_refs)\n",
        "\n",
        "    if n_hard > 0:\n",
        "        bucket_final.loc[hard_refs] = 5\n",
        "\n",
        "    free = sub.loc[~pd.Index(refs).isin(hard_refs)].copy()\n",
        "    n_free = len(free)\n",
        "\n",
        "    if n_free == 0:\n",
        "        continue\n",
        "\n",
        "    if n_hard > targets[5]:\n",
        "        extra = n_hard - targets[5]\n",
        "        targets[5] = n_hard\n",
        "        for b in [4, 3, 2, 1, 0]:\n",
        "            if extra <= 0:\n",
        "                break\n",
        "            take = min(targets[b], extra)\n",
        "            targets[b] -= take\n",
        "            extra -= take\n",
        "    else:\n",
        "        targets[5] = targets[5] - n_hard\n",
        "\n",
        "    total_needed = sum(targets.values())\n",
        "    if total_needed != n_free:\n",
        "        diff = n_free - total_needed\n",
        "        targets[4] = max(0, targets[4] + diff)\n",
        "\n",
        "    free = free.sort_values(\"score_ref\", ascending=True)\n",
        "\n",
        "    start = 0\n",
        "    for b in BUCKETS:\n",
        "        cnt = targets[b]\n",
        "        if cnt <= 0:\n",
        "            continue\n",
        "        end = start + cnt\n",
        "        bucket_final.loc[free.index[start:end]] = b\n",
        "        start = end\n",
        "\n",
        "mask_na = bucket_final.isna()\n",
        "if mask_na.any():\n",
        "    tmp = ref_table.loc[mask_na].sort_values(\"score_ref\", ascending=True)\n",
        "    ranks = tmp[\"score_ref\"].rank(method=\"first\")\n",
        "    q = pd.qcut(ranks, q=6, labels=False, duplicates=\"drop\")\n",
        "    n_q = int(q.max() + 1) if q.notna().any() else 1\n",
        "    if n_q > 1:\n",
        "        q_scaled = np.floor(q * (5 / (n_q - 1))).astype(int)\n",
        "    else:\n",
        "        q_scaled = pd.Series(0, index=tmp.index, dtype=int)\n",
        "    bucket_final.loc[tmp.index] = q_scaled.clip(0, 5).astype(\"Int64\")\n",
        "    bucket_final.loc[list(refs_hard_5)] = 5\n",
        "\n",
        "ref_table[\"Bucket_ref\"] = bucket_final.astype(\"Int64\")\n",
        "\n",
        "# =========================================================\n",
        "# 8) PEGAR A TODAS LAS FILAS (UNA REFERENCIA = UN BUCKET)\n",
        "# =========================================================\n",
        "df_timeline_final = df_timeline_final.merge(\n",
        "    ref_table[[\"Bucket_ref\"]],\n",
        "    left_on=\"Referencia\",\n",
        "    right_index=True,\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "df_timeline_final[\"Bucket\"] = df_timeline_final[\"Bucket_ref\"].astype(\"Int64\")\n",
        "\n",
        "df_timeline_final.drop(\n",
        "    columns=[\"Bucket_ref\", \"Bucket_row\", \"_banco_rank\", \"ratio_ahorro_db\"],\n",
        "    inplace=True,\n",
        "    errors=\"ignore\",\n",
        ")\n",
        "\n",
        "df_timeline_final[\"Bucket\"] = df_timeline_final[\"Bucket\"].astype(int).clip(0, 5).astype(\"Int64\")\n"
      ],
      "metadata": {
        "id": "-4b7yEI9qp6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final"
      ],
      "metadata": {
        "id": "0pS0Aw5a4OBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1) Borrar columnas (no falla si alguna no existe)\n",
        "df_base_funnel = df_base_funnel.drop(columns=[\"_cat_all_norm\", \"cat_norm\"], errors=\"ignore\")\n",
        "\n",
        "# 2) Crear mapeo √∫nico Id deuda -> Bucket (sin duplicar filas)\n",
        "#    Si hay repetidos, se queda con el primer Bucket no nulo (y si todos son nulos, queda nulo)\n",
        "bucket_map = (\n",
        "    df_timeline_final[[\"Id deuda\", \"Bucket\"]]\n",
        "    .dropna(subset=[\"Id deuda\"])\n",
        "    .sort_values(\"Id deuda\")\n",
        "    .drop_duplicates(subset=[\"Id deuda\"], keep=\"first\")\n",
        ")\n",
        "\n",
        "# 3) Agregar Bucket a df_base_funnel sin crear filas nuevas (LEFT JOIN)\n",
        "df_base_funnel = df_base_funnel.merge(\n",
        "    bucket_map,\n",
        "    on=\"Id deuda\",\n",
        "    how=\"left\",\n",
        "    validate=\"m:1\"  # muchas en df_base_funnel -> 1 en bucket_map\n",
        ")"
      ],
      "metadata": {
        "id": "zPHGQ22zCZBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_timeline_final.info()"
      ],
      "metadata": {
        "id": "uTn6yAu7vPyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_base_funnel.info()"
      ],
      "metadata": {
        "id": "S3_2cCdE8xoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_base_funnel"
      ],
      "metadata": {
        "id": "EpvyYnuRyqq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ====== AJUSTA ESTOS NOMBRES SI ES NECESARIO ======\n",
        "COL_ID = \"Id deuda\"       # o \"ID_DEUDA\", \"id_deuda\", etc.\n",
        "COL_REF = \"Referencia\"    # o \"REFERENCIA\", etc.\n",
        "\n",
        "# Categor√≠as\n",
        "COL_BASE_CAT = \"CATEGORIA_PRED_ultima\"\n",
        "COL_TIME_CAT = \"CATEGORIA_PRED\"\n",
        "\n",
        "# ====== 1) Filtrar LIQUIDADO en ambos ======\n",
        "base_liq = df_base_funnel.loc[df_base_funnel[COL_BASE_CAT].eq(\"Liquidado\")].copy()\n",
        "time_liq = df_timeline_final.loc[df_timeline_final[COL_TIME_CAT].eq(\"Liquidado\")].copy()\n",
        "\n",
        "# ====== 2) Ver LIQUIDADOS que est√°n en timeline pero NO aparecen en base (por ID) ======\n",
        "ids_base = set(base_liq[COL_ID].dropna().astype(str))\n",
        "ids_time = set(time_liq[COL_ID].dropna().astype(str))\n",
        "\n",
        "ids_liq_faltan_en_base = sorted(ids_time - ids_base)\n",
        "\n",
        "df_ids_faltan = (\n",
        "    time_liq.loc[time_liq[COL_ID].astype(str).isin(ids_liq_faltan_en_base),\n",
        "                 [COL_ID, COL_REF, COL_TIME_CAT]]\n",
        "    .drop_duplicates()\n",
        "    .sort_values([COL_ID, COL_REF], na_position=\"last\")\n",
        ")\n",
        "\n",
        "print(\"Conteo Liquidado base:\", len(base_liq))\n",
        "print(\"Conteo Liquidado timeline:\", len(time_liq))\n",
        "print(\"IDs Liquidado en timeline que NO est√°n en base_liq:\", len(ids_liq_faltan_en_base))\n",
        "\n",
        "# ====== 3) Lo mismo por Referencia ======\n",
        "refs_base = set(base_liq[COL_REF].dropna().astype(str))\n",
        "refs_time = set(time_liq[COL_REF].dropna().astype(str))\n",
        "\n",
        "refs_liq_faltan_en_base = sorted(refs_time - refs_base)\n",
        "\n",
        "df_refs_faltan = (\n",
        "    time_liq.loc[time_liq[COL_REF].astype(str).isin(refs_liq_faltan_en_base),\n",
        "                 [COL_REF, COL_ID, COL_TIME_CAT]]\n",
        "    .drop_duplicates()\n",
        "    .sort_values([COL_REF, COL_ID], na_position=\"last\")\n",
        ")\n",
        "\n",
        "print(\"Referencias Liquidado en timeline que NO est√°n en base_liq:\", len(refs_liq_faltan_en_base))\n",
        "\n",
        "# ====== 4) (Opcional) Caso m√°s estricto: existen en base, pero NO est√°n como Liquidado en base ======\n",
        "# IDs que est√°n liquidado en timeline, pero en base aparecen con otra categor√≠a (o m√∫ltiples)\n",
        "base_map = (\n",
        "    df_base_funnel[[COL_ID, COL_BASE_CAT]]\n",
        "    .dropna(subset=[COL_ID])\n",
        "    .assign(**{COL_ID: df_base_funnel[COL_ID].astype(str)})\n",
        ")\n",
        "\n",
        "# ids de timeline liquidado que s√≠ existen en base pero base no los tiene liquidado (en ninguna fila)\n",
        "ids_existentes_en_base = ids_time.intersection(set(base_map[COL_ID]))\n",
        "ids_base_liq = set(base_liq[COL_ID].dropna().astype(str))\n",
        "\n",
        "ids_liq_en_time_pero_no_liq_en_base = sorted(ids_existentes_en_base - ids_base_liq)\n",
        "\n",
        "df_ids_no_liq_en_base = (\n",
        "    df_timeline_final.loc[\n",
        "        df_timeline_final[COL_TIME_CAT].eq(\"Liquidado\") &\n",
        "        df_timeline_final[COL_ID].dropna().astype(str).isin(ids_liq_en_time_pero_no_liq_en_base),\n",
        "        [COL_ID, COL_REF, COL_TIME_CAT]\n",
        "    ]\n",
        "    .drop_duplicates()\n",
        ")\n",
        "\n",
        "print(\"IDs Liquidado en timeline que existen en base, pero base NO los tiene como Liquidado:\",\n",
        "      len(ids_liq_en_time_pero_no_liq_en_base))\n",
        "\n",
        "# ====== RESULTADOS ======\n",
        "# df_ids_faltan -> \"Liquidado\" en timeline pero no en base_liq (por ID)\n",
        "# df_refs_faltan -> \"Liquidado\" en timeline pero no en base_liq (por Referencia)\n",
        "# df_ids_no_liq_en_base -> \"Liquidado\" en timeline y existe en base, pero base no lo marca Liquidado"
      ],
      "metadata": {
        "id": "ZV3OKFttnt8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ids_faltan"
      ],
      "metadata": {
        "id": "tPQQSoZ2ppv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ids_no_liq_en_base"
      ],
      "metadata": {
        "id": "qOvskH_Np6C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Subir dataframe"
      ],
      "metadata": {
        "id": "K3BHiR9sSxNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# TIMELINE -> SUBIDA COMPLETA (robusta)  ‚úÖ\n",
        "# - Con retry + backoff\n",
        "# - Con chunks fallback\n",
        "# - FIX: worksheet.update() con named args (compat GitHub/Colab)\n",
        "# =========================================================\n",
        "\n",
        "import os, json, re, time, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "from google.oauth2.service_account import Credentials\n",
        "from gspread.exceptions import APIError, WorksheetNotFound\n",
        "\n",
        "RETRIABLE_CODES = [\"[500]\", \"[502]\", \"[503]\", \"[504]\", \"[429]\"]\n",
        "\n",
        "def _retry(fn, label=\"\", tries=10, base_sleep=1.5, jitter=0.6, max_sleep=45):\n",
        "    last_err = None\n",
        "    for i in range(tries):\n",
        "        try:\n",
        "            return fn()\n",
        "        except APIError as e:\n",
        "            last_err = e\n",
        "            msg = str(e)\n",
        "            if any(c in msg for c in RETRIABLE_CODES):\n",
        "                extra = 6.0 if (\"resize\" in label.lower() or \"update\" in label.lower()) else 0.0\n",
        "                sleep_s = min((base_sleep + extra) * (2 ** i) + random.uniform(0, jitter), max_sleep)\n",
        "                print(f\"[RETRY {i+1}/{tries}] {label} -> {msg[:120]}... sleep {sleep_s:.1f}s\")\n",
        "                time.sleep(sleep_s)\n",
        "                continue\n",
        "            raise\n",
        "    raise last_err\n",
        "\n",
        "def _robust_json_loads(s: str) -> dict:\n",
        "    if not isinstance(s, str):\n",
        "        raise TypeError(\"Se esperaba string para parsear JSON\")\n",
        "    s0 = s.strip()\n",
        "    if (s0.startswith('\"') and s0.endswith('\"')) or (s0.startswith(\"'\") and s0.endswith(\"'\")):\n",
        "        s0 = s0[1:-1].strip()\n",
        "    try:\n",
        "        return json.loads(s0)\n",
        "    except Exception:\n",
        "        pass\n",
        "    s1 = s0.replace(\"\\\\n\", \"\\n\")\n",
        "    try:\n",
        "        return json.loads(s1)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    def fix_private_key(text: str) -> str:\n",
        "        m = re.search(r'\"private_key\"\\s*:\\s*\"([\\s\\S]*?)\"\\s*,\\s*\"client_email\"', text)\n",
        "        if not m:\n",
        "            return text\n",
        "        pk = m.group(1)\n",
        "        pk_fixed = pk.replace(\"\\n\", \"\\\\n\")\n",
        "        return text.replace(pk, pk_fixed)\n",
        "\n",
        "    s2 = fix_private_key(s0)\n",
        "    return json.loads(s2)\n",
        "\n",
        "def load_service_account_info():\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        mi_json = userdata.get(\"MI_JSON\")\n",
        "        if mi_json:\n",
        "            if isinstance(mi_json, dict):\n",
        "                return mi_json\n",
        "            return _robust_json_loads(mi_json)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    sa = os.getenv(\"GOOGLE_SERVICE_ACCOUNT_JSON\") or os.getenv(\"MI_JSON\")\n",
        "    if not sa:\n",
        "        raise ValueError(\"‚ùå Faltan credenciales: define GOOGLE_SERVICE_ACCOUNT_JSON (o MI_JSON).\")\n",
        "    return _robust_json_loads(sa)\n",
        "\n",
        "def _prepare_df_for_sheets(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_out = df.copy()\n",
        "    df_out = df_out.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    for c in df_out.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df_out[c]):\n",
        "            df_out[c] = df_out[c].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    df_out = df_out.where(pd.notna(df_out), \"\")\n",
        "    return df_out\n",
        "\n",
        "def _upload_chunked(ws, df_out: pd.DataFrame, sheet_name: str, chunk_size: int = 3000, pause_s: float = 0.6):\n",
        "    ncols = max(len(df_out.columns), 1)\n",
        "    header = [df_out.columns.tolist()]\n",
        "\n",
        "    _retry(lambda: ws.update(range_name=\"A1\", values=header), label=f\"update header {sheet_name}\")\n",
        "\n",
        "    data = df_out.values.tolist()\n",
        "    start_row = 2\n",
        "    last_col_letter = gspread.utils.rowcol_to_a1(1, ncols).replace(\"1\", \"\")\n",
        "\n",
        "    for i in range(0, len(data), chunk_size):\n",
        "        chunk = data[i:i + chunk_size]\n",
        "        end_row = start_row + len(chunk) - 1\n",
        "        cell_range = f\"A{start_row}:{last_col_letter}{end_row}\"\n",
        "\n",
        "        _retry(\n",
        "            lambda cr=cell_range, ch=chunk: ws.update(\n",
        "                range_name=cr,\n",
        "                values=ch,\n",
        "                value_input_option=\"USER_ENTERED\",\n",
        "            ),\n",
        "            label=f\"update chunk {sheet_name} rows {start_row}-{end_row}\"\n",
        "        )\n",
        "\n",
        "        start_row = end_row + 1\n",
        "        time.sleep(pause_s)\n",
        "\n",
        "def upload_timeline_full(gc, spreadsheet_id: str, df_timeline_final: pd.DataFrame, sheet_name=\"Timeline\"):\n",
        "    spreadsheet = _retry(lambda: gc.open_by_key(spreadsheet_id), label=\"open spreadsheet\")\n",
        "    df_out = _prepare_df_for_sheets(df_timeline_final)\n",
        "\n",
        "    nrows = max(len(df_out) + 1, 2)\n",
        "    ncols = max(len(df_out.columns), 1)\n",
        "\n",
        "    def _get_or_create_ws():\n",
        "        try:\n",
        "            return spreadsheet.worksheet(sheet_name)\n",
        "        except WorksheetNotFound:\n",
        "            return spreadsheet.add_worksheet(title=sheet_name, rows=\"100\", cols=str(ncols))\n",
        "\n",
        "    ws = _retry(_get_or_create_ws, label=f\"get/create worksheet {sheet_name}\")\n",
        "\n",
        "    # Timeline s√≠ se puede reescribir completo\n",
        "    _retry(lambda: ws.clear(), label=f\"clear {sheet_name}\")\n",
        "\n",
        "    try:\n",
        "        _retry(lambda: ws.resize(rows=1000, cols=ncols), label=f\"resize small {sheet_name}\", tries=10, base_sleep=2.0)\n",
        "        _retry(lambda: ws.resize(rows=nrows, cols=ncols), label=f\"resize final {sheet_name}\", tries=10, base_sleep=2.0)\n",
        "    except APIError as e:\n",
        "        print(f\"‚ö†Ô∏è Resize fall√≥ en '{sheet_name}'. Sigo sin resize. {str(e)[:140]}...\")\n",
        "\n",
        "    try:\n",
        "        _retry(\n",
        "            lambda: set_with_dataframe(ws, df_out, include_index=False, include_column_header=True, resize=False),\n",
        "            label=f\"set_with_dataframe {sheet_name}\",\n",
        "            tries=8,\n",
        "            base_sleep=1.5\n",
        "        )\n",
        "        return True, f\"‚úÖ {sheet_name}: {len(df_out):,} filas x {ncols} cols (set_with_dataframe)\"\n",
        "    except APIError as e:\n",
        "        print(f\"‚ö†Ô∏è set_with_dataframe fall√≥ en '{sheet_name}'. Paso a chunks. {str(e)[:140]}...\")\n",
        "\n",
        "    _upload_chunked(ws, df_out, sheet_name, chunk_size=3000, pause_s=0.6)\n",
        "    return True, f\"‚úÖ {sheet_name}: {len(df_out):,} filas x {ncols} cols (chunked)\"\n",
        "\n",
        "\n",
        "# --------- AUTH + RUN ----------\n",
        "SERVICE_ACCOUNT_INFO = load_service_account_info()\n",
        "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
        "creds = Credentials.from_service_account_info(SERVICE_ACCOUNT_INFO, scopes=SCOPES)\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "SPREADSHEET_ID = \"1Bm1wjsfXdNDFrFTStQJHkERC08Eo21BwjZnu-WncibY\"\n",
        "\n",
        "ok_t, msg_t = upload_timeline_full(gc, SPREADSHEET_ID, df_timeline_final, \"Timeline\")\n",
        "print(msg_t)"
      ],
      "metadata": {
        "id": "zRFI6lRbw79l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# FUNNEL -> UPSERT POR \"Id deuda\" ‚úÖ\n",
        "# - NO borra la hoja\n",
        "# - Lee lo existente en Sheets\n",
        "# - Por cada Id deuda:\n",
        "#     - si existe y alguna columna distinta -> actualiza la fila completa\n",
        "#     - si no existe -> agrega al final\n",
        "# - FIX: worksheet.update() con named args (compat GitHub/Colab)\n",
        "# =========================================================\n",
        "\n",
        "import os, json, re, time, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gspread\n",
        "from google.oauth2.service_account import Credentials\n",
        "from gspread.exceptions import APIError, WorksheetNotFound\n",
        "\n",
        "RETRIABLE_CODES = [\"[500]\", \"[502]\", \"[503]\", \"[504]\", \"[429]\"]\n",
        "\n",
        "def _retry(fn, label=\"\", tries=10, base_sleep=1.5, jitter=0.6, max_sleep=45):\n",
        "    last_err = None\n",
        "    for i in range(tries):\n",
        "        try:\n",
        "            return fn()\n",
        "        except APIError as e:\n",
        "            last_err = e\n",
        "            msg = str(e)\n",
        "            if any(c in msg for c in RETRIABLE_CODES):\n",
        "                sleep_s = min((base_sleep) * (2 ** i) + random.uniform(0, jitter), max_sleep)\n",
        "                print(f\"[RETRY {i+1}/{tries}] {label} -> {msg[:120]}... sleep {sleep_s:.1f}s\")\n",
        "                time.sleep(sleep_s)\n",
        "                continue\n",
        "            raise\n",
        "    raise last_err\n",
        "\n",
        "def _robust_json_loads(s: str) -> dict:\n",
        "    if not isinstance(s, str):\n",
        "        raise TypeError(\"Se esperaba string para parsear JSON\")\n",
        "    s0 = s.strip()\n",
        "    if (s0.startswith('\"') and s0.endswith('\"')) or (s0.startswith(\"'\") and s0.endswith(\"'\")):\n",
        "        s0 = s0[1:-1].strip()\n",
        "    try:\n",
        "        return json.loads(s0)\n",
        "    except Exception:\n",
        "        pass\n",
        "    s1 = s0.replace(\"\\\\n\", \"\\n\")\n",
        "    try:\n",
        "        return json.loads(s1)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    def fix_private_key(text: str) -> str:\n",
        "        m = re.search(r'\"private_key\"\\s*:\\s*\"([\\s\\S]*?)\"\\s*,\\s*\"client_email\"', text)\n",
        "        if not m:\n",
        "            return text\n",
        "        pk = m.group(1)\n",
        "        pk_fixed = pk.replace(\"\\n\", \"\\\\n\")\n",
        "        return text.replace(pk, pk_fixed)\n",
        "\n",
        "    s2 = fix_private_key(s0)\n",
        "    return json.loads(s2)\n",
        "\n",
        "def load_service_account_info():\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        mi_json = userdata.get(\"MI_JSON\")\n",
        "        if mi_json:\n",
        "            if isinstance(mi_json, dict):\n",
        "                return mi_json\n",
        "            return _robust_json_loads(mi_json)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    sa = os.getenv(\"GOOGLE_SERVICE_ACCOUNT_JSON\") or os.getenv(\"MI_JSON\")\n",
        "    if not sa:\n",
        "        raise ValueError(\"‚ùå Faltan credenciales: define GOOGLE_SERVICE_ACCOUNT_JSON (o MI_JSON).\")\n",
        "    return _robust_json_loads(sa)\n",
        "\n",
        "def _prepare_df_for_sheets(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_out = df.copy()\n",
        "    df_out = df_out.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    for c in df_out.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df_out[c]):\n",
        "            # datetime64[ns] y datetime64[ns, tz]\n",
        "            df_out[c] = pd.to_datetime(df_out[c], errors=\"coerce\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    df_out = df_out.where(pd.notna(df_out), \"\")\n",
        "    return df_out\n",
        "\n",
        "def _df_to_str_matrix(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normaliza para comparar contra Sheets (que casi siempre devuelve strings).\n",
        "    \"\"\"\n",
        "    out = df[cols].copy()\n",
        "    for c in cols:\n",
        "        # todo como string comparable; \"\" para vac√≠os\n",
        "        out[c] = out[c].astype(str)\n",
        "        out.loc[out[c].isin([\"nan\", \"NaT\", \"<NA>\", \"None\"]), c] = \"\"\n",
        "    return out\n",
        "\n",
        "def _get_or_create_ws(spreadsheet, sheet_name: str, ncols: int):\n",
        "    try:\n",
        "        return spreadsheet.worksheet(sheet_name)\n",
        "    except WorksheetNotFound:\n",
        "        return spreadsheet.add_worksheet(title=sheet_name, rows=\"100\", cols=str(ncols))\n",
        "\n",
        "def _write_header_if_needed(ws, cols: list[str]):\n",
        "    values = _retry(lambda: ws.get_all_values(), label=\"get_all_values header check\")\n",
        "    if not values:\n",
        "        _retry(lambda: ws.update(range_name=\"A1\", values=[cols]), label=\"write header (empty sheet)\")\n",
        "        return\n",
        "\n",
        "    header = values[0]\n",
        "    if header != cols:\n",
        "        # IMPORTANTE: no reordenamos data vieja aqu√≠, solo corregimos encabezado.\n",
        "        # Si tu hoja tuviera columnas distintas, esto te lo hace evidente.\n",
        "        print(\"‚ö†Ô∏è Header en Sheets distinto al DF. Se sobrescribe header con el del DF.\")\n",
        "        print(\"   Sheets:\", header)\n",
        "        print(\"   DF    :\", cols)\n",
        "        _retry(lambda: ws.update(range_name=\"A1\", values=[cols]), label=\"rewrite header\")\n",
        "\n",
        "def _batch_update_rows(ws, start_col_letter: str, end_col_letter: str, row_blocks: list[tuple[int,int,list[list[str]]]]):\n",
        "    \"\"\"\n",
        "    row_blocks: [(start_row, end_row, values_matrix)]\n",
        "    \"\"\"\n",
        "    for (r1, r2, mat) in row_blocks:\n",
        "        rng = f\"{start_col_letter}{r1}:{end_col_letter}{r2}\"\n",
        "        _retry(\n",
        "            lambda rr=rng, mm=mat: ws.update(range_name=rr, values=mm, value_input_option=\"USER_ENTERED\"),\n",
        "            label=f\"update rows {r1}-{r2}\"\n",
        "        )\n",
        "        time.sleep(0.4)\n",
        "\n",
        "def _make_consecutive_blocks(rownums_sorted: list[int], values_by_rownum: dict[int, list[str]]):\n",
        "    \"\"\"\n",
        "    Agrupa filas consecutivas para reducir llamadas a la API.\n",
        "    Retorna [(start_row, end_row, matrix_values)]\n",
        "    \"\"\"\n",
        "    blocks = []\n",
        "    if not rownums_sorted:\n",
        "        return blocks\n",
        "\n",
        "    start = prev = rownums_sorted[0]\n",
        "    mat = [values_by_rownum[start]]\n",
        "\n",
        "    for r in rownums_sorted[1:]:\n",
        "        if r == prev + 1:\n",
        "            mat.append(values_by_rownum[r])\n",
        "            prev = r\n",
        "        else:\n",
        "            blocks.append((start, prev, mat))\n",
        "            start = prev = r\n",
        "            mat = [values_by_rownum[r]]\n",
        "    blocks.append((start, prev, mat))\n",
        "    return blocks\n",
        "\n",
        "def upload_funnel_upsert_by_id(gc, spreadsheet_id: str, df_base_funnel: pd.DataFrame, sheet_name=\"Funnel\"):\n",
        "    spreadsheet = _retry(lambda: gc.open_by_key(spreadsheet_id), label=\"open spreadsheet\")\n",
        "    df_out = _prepare_df_for_sheets(df_base_funnel)\n",
        "\n",
        "    # Asegurar columnas EXACTAS y en el orden del DF\n",
        "    cols = df_out.columns.tolist()\n",
        "    if \"Id deuda\" not in cols:\n",
        "        raise ValueError(\"‚ùå df_base_funnel no tiene la columna 'Id deuda'.\")\n",
        "\n",
        "    ncols = len(cols)\n",
        "    ws = _retry(lambda: _get_or_create_ws(spreadsheet, sheet_name, ncols), label=f\"get/create ws {sheet_name}\")\n",
        "\n",
        "    # Header\n",
        "    _write_header_if_needed(ws, cols)\n",
        "\n",
        "    # Leer todo lo existente\n",
        "    values = _retry(lambda: ws.get_all_values(), label=f\"get_all_values {sheet_name}\")\n",
        "\n",
        "    if len(values) <= 1:\n",
        "        # Hoja vac√≠a (o s√≥lo header): subir todo sin borrar\n",
        "        print(\"‚ÑπÔ∏è Funnel vac√≠o (o s√≥lo header). Hago carga inicial por append en chunks.\")\n",
        "        data_all = _df_to_str_matrix(df_out, cols).values.tolist()\n",
        "        for i in range(0, len(data_all), 1000):\n",
        "            chunk = data_all[i:i+1000]\n",
        "            _retry(lambda ch=chunk: ws.append_rows(ch, value_input_option=\"USER_ENTERED\"), label=f\"append_rows init {i}-{i+len(chunk)-1}\")\n",
        "            time.sleep(0.7)\n",
        "        return True, f\"‚úÖ {sheet_name}: carga inicial {len(df_out):,} filas x {ncols} cols (append)\"\n",
        "\n",
        "    sheet_header = values[0]\n",
        "    sheet_rows = values[1:]\n",
        "\n",
        "    # Construir DF de Sheets\n",
        "    df_sheet = pd.DataFrame(sheet_rows, columns=sheet_header)\n",
        "\n",
        "    # Si por cualquier raz√≥n Sheets no trae todas las columnas, nos quedamos con intersecci√≥n\n",
        "    missing_in_sheet = [c for c in cols if c not in df_sheet.columns]\n",
        "    if missing_in_sheet:\n",
        "        print(\"‚ö†Ô∏è En Sheets faltan estas columnas vs DF (ojo):\", missing_in_sheet)\n",
        "        # Igual seguimos, pero esas columnas las tratamos como \"\" al comparar\n",
        "        for c in missing_in_sheet:\n",
        "            df_sheet[c] = \"\"\n",
        "\n",
        "    # Normalizar Id deuda como string (clave)\n",
        "    df_sheet[\"Id deuda\"] = df_sheet[\"Id deuda\"].astype(str)\n",
        "    df_py_cmp = _df_to_str_matrix(df_out, cols)\n",
        "    df_py_cmp[\"Id deuda\"] = df_py_cmp[\"Id deuda\"].astype(str)\n",
        "\n",
        "    # Map id -> row_number en Sheets (row 2 = primer dato)\n",
        "    id_to_rownum = {}\n",
        "    for i, v in enumerate(df_sheet[\"Id deuda\"].tolist(), start=2):\n",
        "        if v != \"\":\n",
        "            # si hay duplicados, nos quedamos con el primero (puedes cambiarlo si prefieres el √∫ltimo)\n",
        "            if v not in id_to_rownum:\n",
        "                id_to_rownum[v] = i\n",
        "\n",
        "    ids_sheet = set(id_to_rownum.keys())\n",
        "    ids_py = set(df_py_cmp[\"Id deuda\"].tolist())\n",
        "\n",
        "    ids_common = ids_py.intersection(ids_sheet)\n",
        "    ids_new = sorted(list(ids_py - ids_sheet))\n",
        "\n",
        "    # Indexar para comparaci√≥n r√°pida\n",
        "    df_sheet_cmp = _df_to_str_matrix(df_sheet, cols)\n",
        "    df_sheet_cmp[\"Id deuda\"] = df_sheet_cmp[\"Id deuda\"].astype(str)\n",
        "\n",
        "    sheet_by_id = df_sheet_cmp.set_index(\"Id deuda\", drop=False)\n",
        "    py_by_id = df_py_cmp.set_index(\"Id deuda\", drop=False)\n",
        "\n",
        "    # Encontrar ids cambiados (comparando todas las columnas excepto Id deuda)\n",
        "    compare_cols = [c for c in cols if c != \"Id deuda\"]\n",
        "    changed_ids = []\n",
        "    for _id in ids_common:\n",
        "        # si por algo no aparece, saltamos\n",
        "        if _id not in sheet_by_id.index or _id not in py_by_id.index:\n",
        "            continue\n",
        "        a = sheet_by_id.loc[_id, compare_cols]\n",
        "        b = py_by_id.loc[_id, compare_cols]\n",
        "        # a y b son Series\n",
        "        if not a.equals(b):\n",
        "            changed_ids.append(_id)\n",
        "\n",
        "    # Preparar updates por row_number\n",
        "    values_by_rownum = {}\n",
        "    for _id in changed_ids:\n",
        "        rownum = id_to_rownum.get(_id)\n",
        "        if not rownum:\n",
        "            continue\n",
        "        row_values = py_by_id.loc[_id, cols].tolist()\n",
        "        values_by_rownum[rownum] = row_values\n",
        "\n",
        "    rownums_sorted = sorted(values_by_rownum.keys())\n",
        "\n",
        "    start_col_letter = \"A\"\n",
        "    end_col_letter = gspread.utils.rowcol_to_a1(1, ncols).replace(\"1\", \"\")\n",
        "    blocks = _make_consecutive_blocks(rownums_sorted, values_by_rownum)\n",
        "\n",
        "    # Aplicar updates (en bloques)\n",
        "    if blocks:\n",
        "        _batch_update_rows(ws, start_col_letter, end_col_letter, blocks)\n",
        "\n",
        "    # Append nuevos\n",
        "    if ids_new:\n",
        "        rows_new = [py_by_id.loc[_id, cols].tolist() for _id in ids_new]\n",
        "        for i in range(0, len(rows_new), 1000):\n",
        "            chunk = rows_new[i:i+1000]\n",
        "            _retry(lambda ch=chunk: ws.append_rows(ch, value_input_option=\"USER_ENTERED\"), label=f\"append_rows new {i}-{i+len(chunk)-1}\")\n",
        "            time.sleep(0.8)\n",
        "\n",
        "    return True, (\n",
        "        f\"‚úÖ {sheet_name}: \"\n",
        "        f\"{len(changed_ids):,} filas actualizadas, {len(ids_new):,} nuevas (Id deuda). \"\n",
        "        f\"Total DF={len(df_out):,} / Total sheet aprox={len(sheet_rows):,}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# --------- AUTH + RUN ----------\n",
        "SERVICE_ACCOUNT_INFO = load_service_account_info()\n",
        "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
        "creds = Credentials.from_service_account_info(SERVICE_ACCOUNT_INFO, scopes=SCOPES)\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "SPREADSHEET_ID = \"1Bm1wjsfXdNDFrFTStQJHkERC08Eo21BwjZnu-WncibY\"\n",
        "\n",
        "ok_f, msg_f = upload_funnel_upsert_by_id(gc, SPREADSHEET_ID, df_base_funnel, \"Funnel\")\n",
        "print(msg_f)"
      ],
      "metadata": {
        "id": "CodmuakpxBCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}